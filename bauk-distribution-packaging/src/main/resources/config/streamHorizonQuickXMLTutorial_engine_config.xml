<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<config>
	<!-- if they do not exist folders will be created (if permissions allow) -->
	<!-- source directory is directory from which file feeds will be processed -->
	<sourceDirectory>/tmp/stream_horizon/input</sourceDirectory>
	<!-- archive directory is directory into which successfully processed feeds will be moved-->
	<archiveDirectory>/tmp/stream_horizon/archive</archiveDirectory>
	<!-- error directory will contain feeds from source folder which if processing fails for any reason. for error & logging information please look at $ENGINE_HOME/log/engine*log -->
	<errorDirectory>/tmp/stream_horizon/error</errorDirectory>
	<!-- bulk direcory is directory where bulk files will be created. bulk files usually have format of fact table and are inserted into database by use of SQL*Loader, External Tables in case of Oracle or BULK INSERT for MSSQL-->
	<bulkOutputDirectory>/tmp/stream_horizon/bulkOutput</bulkOutputDirectory>
	
	<!-- RDBMS JDBC connection properties -->
	<connectionProperties>	
		<jdbcUrl>jdbc:oracle:thin:@192.168.0.14:1521:orcl</jdbcUrl>
		<jdbcUserName>warehouse</jdbcUserName>
		<jdbcPassword>warehouse</jdbcPassword>
		<!-- jdbcPoolSize should be calculated as: numberOfDimensions + number of parallel jdbc connections + contingency of 10 connections. if you have 10 dimensions in your model and 50 etl threads <feedProcessingThreads> or 50 db threads <bulkLoadProcessingThreads> inserting into fact table connection pool should have minimum of 10+50 dimensions. for contingency set this parameter to 10+50+10. it is important to avoid reusing of jdbc connections by connection pool of fact loader processes as this can be detrimental for performance. default setting is 25. -->
		<jdbcPoolSize>100</jdbcPoolSize>
	</connectionProperties>
	<!-- Character used to enclose string values. Usually this is single quote. Check your database manual -->
	<databaseStringLiteral>'</databaseStringLiteral>
	<!-- Character used to escape databaseStringLiteral (see above). Usually this is double quote. Check your database manual-->
	<databaseStringEscapeLiteral>''</databaseStringEscapeLiteral>
	<factFeeds>
		<!-- setup one <factFeed> element for every file format engine needs to process and insert into data mart <factFeed> name must be unique within xml definition. please use 'full' type of feed as a default unless your feed meets specific delta, repetitive or control criteria-->
	
		<!-- feed can be of type full, repetitive, delta and control -->
		<!-- control feed can not have header and footer and it can have only one line, which will be exposed as context attributes -->
		<!-- feed can be full, delta or repetitive -->
		<!-- for delta feed it is mandatory to set nullString value, see below -->
		<factFeed name="dataFeed" type="full">
			<!-- at startup, prior to starting any processing engine will execute (in defined order) sql or script (shell) commands. please reference all recovery and startup related scripts in this section -->
			<onStartupCommands>
				<!-- all context attributes will be replaced in both sql and shell commands, use normal ${contextAttributeName} notation-->
				<command type="shell">d:/${someAttribute1}.bat</command>
				<command type="sql">update fact_table set key=${someAttribute} where key=-100</command>
				<command type="shell">d:/second.bat</command>
			</onStartupCommands>
			<!--engine has two threadpools:
				1) etl threads (members of etl threadpool) <etlProcessingThreadCount> either:
					a) process feeds from source directory into bulk directory (bulk files are later on picked up by db threads from db threadpool)
					b) process feeds from source directory and insert them directly into database via jdbc without creating bulk files. if using jdbc inserts into fact table set db treadpool threadcount to zero <databaseProcessingThreadCount>.
				2) db threads (members of db threadpool) <databaseProcessingThreadCount> are used to bulk load via BULK INSERT or SQL*Loader or Extenral table or procedure (functions aren't supoprted) call (which invokes external table definition or equivalent).
			-->
			<threadPoolSettings>
				<!-- 0 to turn off feature, any other positive integer to turn it on and use that many threads (per feed) for processing -->
				<!--set number of etl threads. etl threads proces feeds and create bulk files (or execute jdbc batched statemetns) -->
				<etlProcessingThreadCount>20</etlProcessingThreadCount>
				<!--set number of db threads which will concurrently load bulk files into the fact table. If 0 no bulk loading will happen -->
				<databaseProcessingThreadCount>0</databaseProcessingThreadCount>
			</threadPoolSettings>				
			<!-- supply fileName masks for the fees to which this <factFeed> format definition is applicable. feeds will be loaded from source directory <sourceDirectory> -->
			<fileNameMasks>
				<fileNameMask>.*FEED.*done</fileNameMask>
			</fileNameMasks>
			<!-- <nullString> is used only by 'type="delta"' (not 'type="full"') feed parsers -->
			<nullString>!</nullString>
			<!-- class name of custom implementation for feed file name processor. In case when some information should be extracted from file name
			of every input feed file it is possible to provide customized implementation to parse file name and provide new context attributes. 
			Check CustomFeedFileNameProcessor.java in $ENGINE_HOME/plugins/ folder for example -->
			<!--
			<fileNameProcessorClassName>CustomFeedFileNameProcessor</fileNameProcessorClassName>
			-->
			<!-- <delimiterString> defnes delimiter used in your feed -->
			<delimiterString>^$</delimiterString>
			<!---->
			<!--CONTEXT:  every header, footer, body or custom attribute you define in your feed can be referenced from context of ETL engine. context attributes can be used to generate select or insert statements against database or can be used for generation of bulk files.
			
			note: Engine generates and writes out at console 'Implicitly available attributes' context attributes at startup. These attributes can be used for logging and metrics. attribute bulkProcessingThreadID or feedProcessingThreadID which represent etl thread ID and db thread id (<eedProcessingThreads> and <bulkLoadProcessingThreads> respectively) can be used to achieve simultaneous partition/subpartition loading and avoid forming of locks. Please see console at startup for list of all available attributes.
										
			-->
			<!-- define file header structure. if feed has no header set this value to no_header. If there is header and it should not be processed set this value to skip 
				Note that if your feed has attribute which is repeated for every single data record it is ideal candidate to become header attribute -->
			<header process="normal">
				<!--you can utilize custom header record parser class if you need to calculate/define new context parameter based on some of the header or Implicitly available engine attributes or execute any other custom logic. note that this may significantly slow down the engine! java extension will be invoked once for every header record (once per input feed file)-->
				<!-- check CustomHeaderParser.java in $ENGINE_HOME/plugins/ folder for example -->
				<!--
				<headerParserClassName>CustomHeaderParser</headerParserClassName>
				-->
				<!--if header line is defined as line with special character please specify character here. it is assumed that character is delimited from header attributes by <delimiterString>. if header line has no special character erase this tag -->
				<eachLineStartsWithCharacter>0</eachLineStartsWithCharacter>
				<!--define context names for all data attributes (in order as they appear in header record of the feed)-->
				<attributes>
					<attribute name="headerAttributeName_1" />
					<attribute name="headerAttributeName_2" />
					<attribute name="headerAttributeName_3" />
				</attributes>
			</header>
			<!--define data (body) feed record structure-->
			<data process="normal">
				<!--if data line is defined as line with special character pelase specify character here. it is assumed that caracter is delimited from header attributes by <delimiterString> -->			
				<!--if data line is defined as line with special character pelase specify character here. it is assumed that caracter is delimited from data attributes by <delimiterString>. if data line has no special character erase this tag -->			
				<eachLineStartsWithCharacter>1</eachLineStartsWithCharacter>
				<!--you can utilize custom data record parser class if you need to calculate/define new context parameter based on some of the header or Implicitly available engine attributes or execute any other custom logic. note that this may significantly slow down the engine! java extension will be invoked once for every data record (ever data row in every input feed file)-->
				<!-- check DataMapper.java in $ENGINE_HOME/plugins/ folder for example -->				
				<!--
				<feedDataProcessorClassName>DataMapper</feedDataProcessorClassName>
				-->
				<!--define context names for all data attributes (in order as they appear in data record of the feed)-->				
				<attributes>
					<attribute name="dataAttributeName_1" />
					<attribute name="dataAttributeName_2" />
					<attribute name="dataAttributeName_3" />
					<attribute name="dataAttributeName_4" />
					<attribute name="dataAttributeName_5" />
					<attribute name="dataAttributeName_6" />
					<attribute name="dataAttributeName_7" />
				</attributes>
			</data>
			<!--define footer feed record structure-->
			<!-- significance of footer is to hold recordCount (number of expected records within data file and compare them to actual number of processed records from the feed file). if those two don't match engine will fail processing -->
			<footer process="strict">
				<!--if footer line is defined as line with special character pelase specify character here. it is assumed that caracter is delimited from data attributes by <delimiterString>. if footer line has no special character erase this tag -->	
				<eachLineStartsWithCharacter>9</eachLineStartsWithCharacter>
				<!-- at what position will footer 'recordCount' attribute be found. if recordCount is first attribute set 1, if N'th set property to N. note that delimiter for footer is read from <delimiterString> tag -->
				<recordCountAttributePosition>2</recordCountAttributePosition>
			</footer>
			<!-- outputType can be one of following file/none/zip/gz/jdbc -->
			<!-- when set to 'none' if you want to test performance of processing without actually writing bulk file to bulk directory. 'none' value tests performance by eliminating write I/O -->
			<!-- when set to 'zip' then .zip output file will be zip compressed and will have .zip extension added -->
			<!-- when set to 'gz' then .gz output file will be gzip compressed and will have .gz extension added -->
			<!-- when set to  'jdbc' you must specify types for all attributes in <bulkLoadFormatDefinition> tag and bulk records will be inserted using JDBC -->
			<bulkLoadDefinition outputType="file">
				<!-- this section is executed by db loading threads <bulkLoadProcessingThreads> -->
				<!-- <bulkLoadOutputExtension> defines the extension of bulk output file. db threads will consider only files with given extension for loading into database . for example, if you expect 'zip' files to be loaded change this extension to zip (there is no need to add dot character for example '.zip'). if property is left as null or empty, extension will not be used as part of matching files in the bulk load folder -->
				<bulkLoadOutputExtension></bulkLoadOutputExtension>
				<!-- specifies delimiter to be used when creating bulk output file. Whenever possible use single character delimiter -->
				<bulkLoadFileDelimiter>,</bulkLoadFileDelimiter>
				<!-- sql statement for loading bulk output file. Can be any sql statement, function call or stored procedure invocation -->
				<!-- in case of jdbc loading this statement must be prepared statement listing all attributes that are defined in bulkLoadFormatDefinition section -->	

				<!--EXAMPLES:-->				
					<!-- EXAMPLE 1) function call example:
						 this example shows how function can be called via jdbc. note that imlicitly defined engine context parameter 'bulkProcessingThreadID' is used in order to loadbalance subpartitions of target fact table and avoid locking. also note that another implicit engine context parameter 'bulkFileName' is used as function internaly need to alter table and repoint external table to data file which need be loaded. 
						 
						 prefixing context attributes with ${contextAttributeName} will ensure that engine engine replaces them before issuing the call against RDBMS
						 
						 This is most performant way to insert data into the fact table. note that best results have been acheived when bulk files aren't zipped or gz'ed and when paralelism is high (db threads load into 30-50 subpartitions. no logging mode will usually increase performance by 50%.
					-->
					<bulkLoadInsert>
						<command type="sql">
						CALL schema.functionName(mod(${bulkProcessingThreadID},50),${bulkProcessingThreadID}, '${bulkFileName}' )  
						</command>
					</bulkLoadInsert>	
					<!--EXAMPLE 2) direct jdbc insert example
						for performance reasons 'customized.formattedHeaderDate' customised attribute is used to deliver value which will denote table partition name P_${customized.formattedHeaderDate}. to test you may supply hardcoded value to start with P_200110101
						
						in similar fashion, in order to target right subpartition and enable paralel load of subpartitions, another context custom defined attribute customized.feedProcessingThreadID.modulo is used SP_${customized.feedProcessingThreadID.modulo}
					
						note that insert statement below has 7 question marks and 9 fact table attributes. Engine will replace all context attributes prior to issuing statament against the database. as context attributes ${*} will be replaced with concrete values <bulkLoadFormatDefinition> tag will require definition for only 7 <attributes> tags for fact table. 7 defined attributes will be assigned in listed order to placeholder vaues (replaced with ? sign of sql query).
					-->					
					<!--bulkLoadInsert>
					<command type="sql">
						insert /*+ append_values */ into schema.factTableName 
						subpartition (P_${customized.formattedHeaderDate}_SP_${customized.feedProcessingThreadID.modulo}) 
						( tableAttribute1,  tableAttribute2, tableAttribute3,  tableAttribute4,  tableAttribute5, tableAttribute6,  tableAttribute7,tableAttribute8,  tableAttribute9)
						VALUES (?,?, ${customized.formattedHeaderDate} , ${customized.feedProcessingThreadID.modulo} ,?,?,?,?,?)
						</command>
					</bulkLoadInsert-->	

					<!--EXAMPLE 3) calling  MySQL external script or command line to bulk load the file:
					
					note taht implicit Engine context attributes bulkFilePath and bulkProcessingThreadID are embeded & swapped to the command prior to its execution
					
					note that type of command is 'shell' rather than 'sql'
					-->
				<!--bulkLoadInsert>
				<command type="shell">
					LOAD DATA INFILE '${bulkFilePath}', ${bulkProcessingThreadID} INTO TABLE test_fact FIELDS TERMINATED BY '@@' LINES TERMINATED BY '\n'
					</command>
				</bulkLoadInsert-->	
				
					<!--EXAMPLE 4) calling  MSSQL external script or command line to bulk load a bulk file:
					
					note taht implicit Engine context attributes bulkFileName is embeded & swapped to the command prior to its execution
					
					note that type of command is 'shell' rather than 'sql'
					
					bulkLoadScript.bat should (after variable substitution) execute command similar to:
					CALL SQLCMD -S server\instance -q "BULK INSERT user.[dbo].[factTableName]   FROM 'pathAndFileNameToBeLoaded' WITH (FIELDTERMINATOR =',',ROWTERMINATOR ='0x0a',TABLOCK)"

					-->
				<!--bulkLoadInsert>
					<command type="shell">g:/bulkLoadScript.bat</command>
				</bulkLoadInsert-->					
				
				<bulkLoadFormatDefinition>
					<attributes>
						<!-- 
							attributes can come from Engine context from few places:
								1) data/file feed and should be referenced with 'feed.' prefix
								2) dimension surrogateKey and should be referenced with 'dimension.' prefix 
								3) attributes can be hardcoded values (constants) see example below with 'Y'. if using hardcoded contant as string make sure you put appropriate database string delimiters (quotes in this example)
								4) can be mapped to any context attribute (just use context attribute name, no need for ${} characters 
						 -->
						<!-- order of attributes is important and determines definition of created bulk file (or generated jdbc statement) -->
						<!-- dimension.XYZ means output calculated surrogateKey (ID) for dimension XYZ (based on dimension definition (please see below)) -->
						<!-- datatypes supoprted are can be int/string/float -->
						<attribute name="dimension.dimension1" type="int" />						
						<attribute name="dimension.dimension2" type="int" />
						<attribute name="dimension.dimension3" type="int" />
						<!-- hardcoded/constant attributes. will be copied directly to output. if string don't forget to supply string delimiters -->
						<attribute type="int">-1</attribute>
						<attribute type="string">Y</attribute>
						<attribute type="string">N</attribute>
						<attribute type="string">X</attribute>
						<!-- copy directly from input feed, as is -->
						<attribute name="feed.dataAttributeName_7" type="float" /> <!-- this is an example of mapping value of measure directly from the feed-->
						<!-- attribute value will be looked in context, can be implicit or custom -->
						<attribute name="customized.attribute.from.context" type="int" />
					</attributes>
				</bulkLoadFormatDefinition>
				<!-- renaming bulk file after it has been generated by the engine. If you need this functionality:
						-if bulk output file is to be renamed after it is created (but before loaded by db threads) set this value
						-any value from context can be used as ${} placeholder and engine will replace it before renaming 
						-renamed file will be left in <bulkOutputDirectory> - you can not move file to other folder using this functionality
						-if no renaming is required leave this tag empty or delete it
						-one possible example of this pattern is myFile_${feedInputFileName}_${feedInputFileProcessingStartedTimestamp}.txt 
				-->
				<output-file-name-pattern></output-file-name-pattern>
				<!-- executed by db threads -->
				<afterBulkLoadSuccess>
					<!-- statements to be invoked after bulkLoadInsertStatement is successfully invoked. db thread which does bulk loading executes given commands (if any)! commands are executed in given order and can receive any of engine context parameters. Any context attribute will be replaced with its value -->
					<command type="shell">d:/first.bat ${someCtxAttributePassedAsParameter}</command>
					<command type="sql">CALL simpleproc1(${someContextAttribute1})</command>
					<command type="sql">CALL simpleproc2('${firstCtxAttr}',${secondCtxAttr})</command>
					<command type="shell">d:/second.bat</command>
				</afterBulkLoadSuccess>
				<onBulkLoadFailure>
					<!-- by default files are moved to error folder on processing failure -->
					<!-- statements to be invoked after loading of bulk data file has failed (if it fails). db thread which does bulk loading executes given commands (if any)! commands are executed in given order and can receive any of engine context parameters -->
					<command type="shell">d:/first.bat</command>
					<command type="sql">CALL simpleproc1(${someContextAttribute1})</command>
					<command type="sql">CALL simpleproc2('${firstCtxAttr}',${secondCtxAttr})</command>
					<command type="shell">d:/second.bat</command>
				</onBulkLoadFailure>
			</bulkLoadDefinition>
			<!-- following is executed by etl (feed processing) thread in the order specified, only if feed processing has failed, think of this as catch{} block in Java -->
			<onFeedProcessingFailure>
				<!-- by default files are moved to error folder on processing failure -->
				<command type="shell">d:/rollback.bat</command>
				<!-- it is possible to use any context attributes here -->
				<command type="sql">UPDATE some table for rollback</command>
				<command type="shell">d:/second_rollback.bat</command>
			</onFeedProcessingFailure>
			
			<!-- <afterFeedProcessingCompletion> is executed by etl threads (feed processing threads) in order specified. This is executed irrespective of sucess/failure of etl thread processing (think of this as finally{} block equivalent in Java) -->
			<afterFeedProcessingCompletion>
				<command type="shell">d:/first.bat</command>
				<!-- any context attributes may be used & substituted -->
					<command type="sql">CALL schema.function('${dataAttributeName_1}', 
															 trunc(to_date('${dataAttributeName_3}','DD/MM/YYYY')), 
															 'hardcodedString',
															 ${dimension1.sk}, /* example of use of surrogateKey value from dimensional cache */
															 to_timestamp('${feedInputfileReceivedDateTime}', 'DD/MM/YYYY HH24:MI:SS.FF3'))
					</command>
				<command type="shell">d:/second.bat</command>
			</afterFeedProcessingCompletion>
		</factFeed>
	</factFeeds>
	<dimensions>
		<!--Dimension definition enables caches (global cache (Infinispan) and local etl thrad caches to derive surrogate keys of dimensions which will be required duiring the insert of data into fact table.
			
				1)surrogate key ('sk' from now on) for every dimension used in feed will be exposed as  context attribute under name ${DIMENSION_NAME.sk}. it will have value of last line in the feed
				2)<exposeLastLineValueInContext> determines whether surrogate key caculated for the last line of every feed should be available in context as attribute named ${DIMENSION_NAME.sk}, default is false
				3)<cachePerThreadEnabled> caching per thread for dimension makes sense only if cardinality for that dimension is low. avoid caching for dimensions of high cardinalities (over million distinct values). or alternatively test both scenarios to derive performance statistiscs
				
				three dimension types will be supported INSERT_ONLY, T1, T2. Currently only type="INSERT_ONLY"  is supported
				
				INSERT_ONLY-mode of operation assumes that all dimensional attributes are part of natural key. if exact match isn't found in cache inser statement will be issued and new generated surrogate key will be used. pelase see below
				
		-->
		<dimension name="dimension1" type="INSERT_ONLY" exposeLastLineValueInContext="false" cachePerThreadEnabled="true">
			<!-- define how many elements can be stored in local cache for this dimension (local etl thread cache (not global (Infinispan cache)).  if not set then default value will be used <dimension.local.cache.default.size>  -->	
			<localCacheMaxSize>10000</localCacheMaxSize>
			<mappedColumns>
				<!-- names of mappedColumns should match attributes in feed or attributes in context. Fastest is if they match attributes in feed -->
				<mappedColumn name="dataAttributeName_1" naturalKey="true" />
				<mappedColumn name="dataAttributeName_2" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingleRecord>
					insert into dimensionTableName1(dimensionSurrogateKey, tableAttribute1, tableAttribute2)
					values (dimensionSequenceName.nextval,'${dataAttributeName_1}','${dataAttributeName_2}')
				</insertSingleRecord>
				<!-- must return single value (surrogate key). Ideally WHERE clause should use all naturalKey columns from <mappedColumns> section -->
				<selectRecordIdentifier>select dimensionSurrogateKey from dimensionTableName1 where tableAttribute1='${dataAttributeName_1}' and tableAttribute2='${dataAttributeName_2}'</selectRecordIdentifier>
				<!--  order is important, first return surrogate key and then all natural keys, in order as defined by <mappedColumns> -->
				<!-- please make sure that cardinality of <preCacheRecords> resultset of sql query below is less than <localCacheMaxSize>. this is to avoid quick eviction of data from the cache. if not certan, please test both small and large <localCacheMaxSize> for dimensions with high cardinality (500K+)-->				
				<preCacheRecords>select dimensionSurrogateKey, tableAttribute1, tableAttribute2 from dimensionTableName1</preCacheRecords>
			</sqlStatements>
		</dimension>
		<dimension name="dimension2" type="INSERT_ONLY" exposeLastLineValueInContext="false" cachePerThreadEnabled="true">		
			<localCacheMaxSize>10000</localCacheMaxSize>
			<mappedColumns>
				<mappedColumn name="dataAttributeName_3" naturalKey="true" />
				<mappedColumn name="dataAttributeName_4" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingleRecord>
					insert into dimensionTableName2(dimensionSurrogateKey, tableAttribute1, tableAttribute2)
					values (dimensionSequenceName.nextval,'${dataAttributeName_1}','${dataAttributeName_2}')
				</insertSingleRecord>
				<selectRecordIdentifier>select dimensionSurrogateKey from dimensionTableName2 where tableAttribute1='${dataAttributeName_1}' and tableAttribute2='${dataAttributeName_2}'</selectRecordIdentifier>
				<preCacheRecords>select dimensionSurrogateKey, tableAttribute1, tableAttribute2 from dimensionTableName2</preCacheRecords>
			</sqlStatements>
		</dimension>		
		<dimension name="dimension3" type="INSERT_ONLY" exposeLastLineValueInContext="false" cachePerThreadEnabled="true">		
			<localCacheMaxSize>10000</localCacheMaxSize>
			<mappedColumns>
				<mappedColumn name="dataAttributeName_5" naturalKey="true" />
				<mappedColumn name="dataAttributeName_6" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingleRecord>
					insert into dimensionTableName3(dimensionSurrogateKey, tableAttribute1, tableAttribute2)
					values (dimensionSequenceName.nextval,'${dataAttributeName_1}','${dataAttributeName_2}')
				</insertSingleRecord>
				<selectRecordIdentifier>select dimensionSurrogateKey from dimensionTableName3 where tableAttribute1='${dataAttributeName_1}' and tableAttribute2='${dataAttributeName_2}'</selectRecordIdentifier>
				<preCacheRecords>select dimensionSurrogateKey, tableAttribute1, tableAttribute2 from dimensionTableName3</preCacheRecords>
			</sqlStatements>
		</dimension>
	</dimensions>
	<properties>
		<!-- should processed feed files be remembered or not (true/false). this setting essentially disables system to process input feed file with the same name twice if set to true -->
		<property name="idempotent.feed.processing">false</property>
		<!-- what is used for caching, choices are Hazelcast or Infinispan, values of parameters should be hazelcast or ispn. if global <cache.provider> doesn't contain lookup value database will be queried (insert will be tried, if fails, adequate key will be read) -->
		<property name="cache.provider">ispn</property>
		<!-- default local cache size for dimensions. Can be overridden for every dimension (see below dimension configuration section). Local cache is size of local collection each etl thread has. etl thread has one cache for every single dimension defined. if cache has no value local etl thread cache will query global <cache.provider> (Infinispan or Hazelcast) -->
		<property name="dimension.local.cache.default.size">5000</property>
		<!-- read buffer size to disk (megabytes) - accepts decimal numbers. adjust this value to be just over the size of your biggest source feed file. if you experience IO bottleneck adjusting to size of 10% up to 100% of source feed file could bring performance benefits -->
		<property name="read.buffer.size.mb">10.0</property>
		<!-- write buffer size to disk (megabytes) - accepts decimal numbers. adjust this value to be just over the size of your biggest source feed file. if you experience IO bottleneck adjusting to size of 10% up to 100% of source feed file could bring performance benefits -->
		<property name="write.buffer.size.mb">10.0</property>
		<!-- whether local cache for dimensions (local cache of etl threads) should be disabled (true/false). -->
		<property name="dimension.local.cache.disable">false</property>
		<!-- is exposing statistics feature off or on (true/false). No need to disable except in case when box is CPU bound -->
		<property name="metrics.off">false</property>
		<!-- jdbc client info - program name, set to empty not to use this feature. Setting embeds client information to simplify database connection audit and tuning -->
		<property name="jdbc.client.info.program"></property>
		<!-- indicates whether files moved from source to archive folder should be renamed (renaming will result in appending date and time to the file name) -->
		<property name="rename.archived.files">false</property>
		<!-- should we detect other running engine instances and print count at startup -->
		<property name="detect.running.engine.instances">false</property>
		<!-- JDBC fetch size used when pre-caching dimension keys. Must be positive integer value. Dimension pre-caching is used by global cache.provider at startup in order to reduce impact on database at runtime and avoid performance degradation. pre-caching populates Infinispan or Hazelcast global cache with dimensional keys & attributes.-->
		<property name="dimension.precache.jdbc.fetch.size">10000</property>
		<!-- whether pre-caching for all dimensions is disabled for both global <cache.provider> (Infinispan) and etl threads <feedProcessingThreads> -->
		<property name="dimension.precaching.disabled">false</property>
		<!-- should processing statistics be written to log file and console as files are processed -->
		<property name="output.processing.statistics">false</property>
		<!-- when persisting NULL value what should actually be written to output (bulk) file, by default this is empty string (no character will be written) -->
		<property name="bulk.output.file.null.value.string"></property>
		<!-- should bulk files be deleted after they are loaded. if set to true, bulk files will be deleted from bulkOutputDirectory -->
		<property name="bulk.delete.files.after.load">true</property>
		<!-- should engine record (internally) submission of every file it tries to bulk load? this setting is used for recovery, in case when engine dies halfway through bulk-loading file it will know that file was submitted for loading beforehand (this info is persisted and it survives engine crash and/or restart) -->
		<property name="bulk.file.record.submission.attempts">true</property>
		<!-- how often (in milliseconds) to poll for input files (feed, bulk). -->
		<property name="file.polling.delay.millis">1000</property>
		<!-- following two properties can be 0 if file system allows atomic renames/moves of files. Data corruption can occur in case someone is writing to files while engine is consuming them. this can be prevented by user (files can be renamed by file writer after writing is done) -->
		<!-- how old must be feed file before it can be accepted for processing. set to 0 to immediately process files -->
		<property name="feed.file.acceptance.timeout.millis">2000</property>
		<!-- how old must be bulk file before it can be accepted for processing. set to 0 to immediately process files -->
		<property name="bulk.file.acceptance.timeout.millis">2000</property>
		<!-- batch size to be used when bulk loading using jdbc. Whenever number of batched values reaches this number it will be executed, committed and batching will start again -->
		<property name="jdbc.bulk.loading.batch.size">10000</property>
		<!-- throughput.testing.mode eliminates read I/O bottleneck while testing. when set to true, etl threads <feedProcessingThreads> will each cache one random file and process it not by reading it from disk but by reading it from repeatedly (while test lasts) from internal buffer. this will give idea of throughput without read I/O bottleneck. it is advisable that source directory contains more files than number of etl threads <feedProcessingThreads> -->
		<property name="throughput.testing.mode">false</property>
		<!-- port to be used by http server for remote commands. If set to 0 then http server will not be started.
			if something else than engine engine has updated database dimensional values that implies that global cache (Infinispan) has incorrect values. 
			you could make engine flush all dimensional caches if you execute http://localhost:21000/flushDimensionCache/?dimension=dimension1
		-->
		<property name="remoting.server.port">21000</property>
		<!-- used for email feature -->
		<property name="email.server.host">emailServerHostName</property>
		<property name="email.server.username"></property>
		<property name="email.server.password"></property>
		<property name="email.server.port">25</property>
		<!-- addressses to which error emails will be sent (coma delimited list of email addresses)-->
		<property name="processing.error.email.recipients">a@b.com,c@d.com</property>
	</properties>
</config>
			
			
			
