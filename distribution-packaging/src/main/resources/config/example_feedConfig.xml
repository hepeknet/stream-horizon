<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<config>
	<!-- if they do not exist folders will be created (if possible) -->
	<sourceDirectory>d:/projects/test/input</sourceDirectory>
	<archiveDirectory>d:/projects/test/archive</archiveDirectory>
	<errorDirectory>d:/projects/test/error</errorDirectory>
	<bulkOutputDirectory>d:/projects/test/bulkOutput</bulkOutputDirectory>
	<properties>
		<!-- should processed feed files be remembered or not (true/false) -->
		<property name="idempotent.feed.processing">false</property>
		<!-- what is used for caching (hazelcast/ispn) -->
		<property name="cache.provider">ispn</property>
		<!-- default local cache size for dimensions. Can be overridden for every dimension (see below) -->
		<property name="dimension.local.cache.default.size">5000</property>
		<!-- read buffer size to disk (megabytes) - accepts decimal numbers -->
		<property name="read.buffer.size.mb">10.0</property>
		<!-- write buffer size to disk (megabytes) - accepts decimal numbers -->
		<property name="write.buffer.size.mb">10.0</property>
		<!-- whether local cache for dimensions should be disabled (true/false) -->
		<property name="dimension.local.cache.disable">false</property>
		<!-- is exposing statistics feature off or on (true/false). No need to disable expect in extreme cases when every CPU cycle is important -->
		<property name="metrics.off">false</property>
		<!-- jdbc client info - program name, set to empty not to use this feature. For Oracle we set v$session.program -->
		<property name="jdbc.client.info.program">ETLBauk</property>
		<!-- whether files moved to archive folder should be renamed (appending date and time) or not (true/false) -->
		<property name="rename.archived.files">false</property>
		<!-- should we detect other running bauk instances and print count at startup -->
		<property name="detect.running.bauk.instances">false</property>
		<!-- JDBC fetch size used when pre-caching dimension keys -->
		<property name="dimension.precache.jdbc.fetch.size">10000</property>
		<!-- should processing statistics be written as files are processed -->
		<property name="output.processing.statistics">false</property>
		<!-- when outputting null value what should actually be written to output file, by default this is empty string - nothing will be written -->
		<property name="bulk.output.file.null.value.string"></property>
		<!-- how often to poll for input files (feed, bulk) -->
		<property name="file.polling.delay.millis">1000</property>
		<!-- following two properties can be 0 if file system allows atomic renames/moves of files. Data corruption can happen in case someone is writing to files while engine is consuming them -->
		<!-- how old must be feed file before it can be accepted for processing. Can be 0 to immediately process files -->
		<property name="feed.file.acceptance.timeout.millis">2000</property>
		<!-- how old must be bulk file before it can be accepted for processing. Can be 0 to immediately process files -->
		<property name="bulk.file.acceptance.timeout.millis">2000</property>
		<!-- batch size to be used when loading bulk using jdbc. Whenever number of batched values reaches this number it will be executed and batching will start again -->
		<property name="jdbc.bulk.loading.batch.size">10000</property>
		<!-- port to be used by http server for remote commands -->
		<property name="remoting.server.port">21000</property>
		<!-- used for email feature -->
		<property name="email.server.host">localhost</property>
		<property name="email.server.username"></property>
		<property name="email.server.password"></property>
		<property name="email.server.port">25</property>
		<!-- where to send error emails -->
		<property name="processing.error.email.recipients">a@b.com, c@d.com</property>
	</properties>
	<connectionProperties>
		<jdbcUrl>jdbc:oracle:thin:@192.168.0.14:1521:orcl</jdbcUrl>
		<jdbcUserName>warehouse</jdbcUserName>
		<jdbcPassword>warehouse</jdbcPassword>
		<jdbcPoolSize>30</jdbcPoolSize>
	</connectionProperties>
	<databaseStringLiteral>'</databaseStringLiteral>
	<databaseStringEscapeLiteral>''</databaseStringEscapeLiteral>
	<factFeeds>
		<!-- feed can be of type full, repetitive, delta and control -->
		<!-- control feed can not have header and footer and it can have only one line, which will be exposed as context attributes -->
		<factFeed name="NOTIF" type="control">
			<!-- executed on startup, before any feed files are processed -->
			<!-- order of commands is important, failure of one command will stop others to execute -->
			<onStartupCommands>
				<command type="shell">d:/first.bat</command>
				<command type="sql">update fact_table set key=1 where key=-100</command>
				<command type="shell">d:/second.bat</command>
			</onStartupCommands>
			<fileNameMasks>
				<!-- masks must be valid Java regex -->
				<fileNameMask>.*NOTIF.*</fileNameMask>
			</fileNameMasks>
			<delimiterString>^~</delimiterString>
			<!-- for header valid values are no_header, normal and skip -->
			<!-- no_header means there is no header line, normal means header will be processed as configured, skip means there is header line but will not be processed -->
			<header process="no_header" />
			<!-- values for processing of data can be normal and no_validation -->
			<!-- normal means that it is required to have validation string at the beginning of every line -->
			<!-- no_validation means that validation string is not present -->
			<data process="no_validation">
				<!-- if not specified then we do not expect line to start with any special character -->
				<eachLineStartsWithCharacter></eachLineStartsWithCharacter>
				<attributes>
					<!-- here we name all attributes that should be found in the feed. Attribute values can be accessed by using ${attributeName} -->
					<attribute name="guid" />
					<attribute name="runGuid" />
					<attribute name="taskCount" />
					<attribute name="fileCount" />
					<attribute name="persistedTaskCount" />
					<attribute name="persistedTradeCount" />
					<attribute name="persistedValidValuesCount" />
					<attribute name="persistedInvalidValuesCount" />
					<attribute name="notPersistedTaskCount" />
					<attribute name="notPersistedTradeCount" />
				</attributes>
			</data>
			<!-- footer process can be skip or strict -->
			<!-- skip means last line will be skipped, strict means that last line will be processed and validity of footer will be checked -->
			<footer process="skip" />
			<bulkLoadDefinition outputType="none" />
			<threadPoolSizes>
				<!-- 0 to turn off feature -->
				<feedProcessingThreads>1</feedProcessingThreads>
				<bulkLoadProcessingThreads>0</bulkLoadProcessingThreads>
			</threadPoolSizes>
		</factFeed>
		<!-- feed can be full, delta or repetitive -->
		<!-- for delta feed it is important to set nullString value, see below -->
		<factFeed name="dataFeed" type="full">
			<fileNameMasks>
				<fileNameMask>.*FEED.*done</fileNameMask>
			</fileNameMasks>
			<!-- null string is used only by delta feed parsers -->
			<nullString>ab</nullString>
			<!--
			<fileNameProcessorClassName>CustomFeedFileNameProcessor</fileNameProcessorClassName>
			-->
			<delimiterString>^$</delimiterString>
			<header process="normal">
				<!--
				<headerParserClassName>CustomHeaderParser</headerParserClassName>
				-->
				<eachLineStartsWithCharacter>0</eachLineStartsWithCharacter>
				<attributes>
					<attribute name="fileUserName" />
					<attribute name="qlVersion" />
					<attribute name="portfolio" />
					<attribute name="eodFlag" />
					<attribute name="location" />
					<attribute name="intradayName" />
					<attribute name="businessDate" />
					<attribute name="rerunVersion" />
					<attribute name="curveType" />
					<attribute name="parisRequestId" />
					<attribute name="guid" />
					<attribute name="rerunGuid" />
					<attribute name="requestTimestamp" />
					<attribute name="requestGuid" />
					<attribute name="runTag" />
				</attributes>
			</header>
			<data process="normal">
				<eachLineStartsWithCharacter>1</eachLineStartsWithCharacter>
				<!--
				<feedDataProcessorClassName>DataMapper</feedDataProcessorClassName>
				-->
				<attributes>
					<attribute name="componentCode" />
					<attribute name="factorCode" />
					<attribute name="underlyingCode" />
					<attribute name="riskUnderlyingCode" />
					<attribute name="gridCellType" />
					<attribute name="gridCellValue" />
					<attribute name="riskCurrencyCode" />
					<attribute name="riskValue" />
					<attribute name="baseCurrencyCode" />
					<attribute name="baseValue" />
					<attribute name="exceptionCode" />
					<attribute name="exceptionDescription" />
					<attribute name="exceptionFlag" />
					<attribute name="sourceName" />
					<attribute name="dealType" />
					<attribute name="dealId" />
					<attribute name="dealVersion" />
					<attribute name="dealWhatIfUID" />
					<attribute name="dealDate" />
					<attribute name="dealAmended" />
					<attribute name="dealState" />
					<attribute name="structureCd" />
					<attribute name="orgCode" />
					<attribute name="productCode" />
					<attribute name="partyName" />
					<attribute name="trade_ql" />
					<attribute name="trade_tds" />
					<attribute name="maturityDate" />
					<attribute name="localCurrencyCode" />
					<attribute name="localValue" />
					<attribute name="pricingModel" />
					<attribute name="curveType" />
				</attributes>
			</data>
			<!-- delimiter for footer values is the same as delimiter for factFeed (data) values -->
			<footer process="strict">
				<!-- character with which every footer line should start -->
				<eachLineStartsWithCharacter>9</eachLineStartsWithCharacter>
				<!-- at what position will footer count attribute be found -->
				<recordCountAttributePosition>2</recordCountAttributePosition>
			</footer>
			<!-- outputType can be one of following file/none/zip/gz/jdbc -->
			<!-- set to none if you want to test performance of processing (but not writing bulk output file) -->
			<!-- if zip then .zip output file will be zip compressed and will have .zip extension added -->
			<!-- if gz then .gz output file will be gzip compressed and will have .gz extension added -->
			<!-- if jdbc then you have to specify types for all attributes in bulkLoadFormatDefinition -->
			<bulkLoadDefinition outputType="file">
				<!-- this section is executed by DB loading threads -->
				<!-- extension of bulk output file. Bulk loader will consider only files with this extension for loading into database -->
				<!-- for example, if you expect .zip files to be loaded change this extension to zip -->
				<!-- no need to add dot character -->
				<!-- can be null or empty, in which case extension will not be used as part of matching files in bulk load folder -->
				<bulkLoadOutputExtension>data</bulkLoadOutputExtension>
				<!-- what delimiter will be used when creating bulk output file. Whenever possible use single character delimiter -->
				<bulkLoadFileDelimiter>,</bulkLoadFileDelimiter>
				<!-- sql statement for loading bulk output file. Can be any sql statement or stored procedure invocation -->
				<!-- in case of jdbc loading this statement must be prepared statement listing all attributes that are defined in bulkLoadFormatDefinition section -->
				<!-- for example insert into FACT(col1, col2, col3) VALUES (?,?,?) -->
				<bulkLoadInsertStatement>LOAD DATA INFILE '${bulkFilePath}', ${bulkProcessingThreadID}
					INTO TABLE test_fact FIELDS TERMINATED BY '@@' LINES TERMINATED BY
					'\n'</bulkLoadInsertStatement>
				<bulkLoadFormatDefinition>
					<attributes>
						<!-- 
							attributes can come from 
								feed (feed. prefix), 
								dimension SK (dimension. prefix), 
								can be constants (no name given) or 
								can be mapped to any context attribute (just use context attribute name, no need for ${} characters 
						 -->
						<!-- order of attributes is important -->
						<!-- dimension.XYZ means output calculated SK for dimension XYZ (based on dimension definition) -->
						<attribute name="dimension.RWH_DEAL_DIM" type="int" />
						<!-- type can be int/string/float -->
						<attribute name="dimension.RWH_PRODUCT_DIM" type="int" />
						<attribute name="dimension.RWH_UNDERLYING_DIM" />
						<attribute name="dimension.RWH_CURVETYPE_DIM" />
						<attribute name="dimension.RWH_DATE_DIM" />
						<attribute name="dimension.RWH_CURRENCY_DIM" />
						<attribute name="dimension.RWH_PARTY_DIM" />
						<attribute name="dimension.RWH_PORTFOLIO_DIM" />
						<attribute name="dimension.RWH_RISK_STATUS_DIM" />
						<!-- constant attributes. Will be copied directly to output -->
						<attribute type="int">-1</attribute>
						<attribute>'Y'</attribute>
						<attribute>'N'</attribute>
						<!-- copy directly from input feed, as is -->
						<attribute name="feed.localValue" />
						<attribute name="feed.baseValue" />
						<attribute name="feed.riskValue" />
						<!-- attribute value will be looked in context, can be implicit or custom -->
						<attribute name="customized.attribute.from.context" />
					</attributes>
				</bulkLoadFormatDefinition>
				<!-- if we want bulk output file to be renamed to something after created (but before loaded) we set this value -->
				<!-- any value from context can be used as ${} placeholder and engine will replace it before renaming -->
				<!-- renamed file will still be left in bulkOutputDirectory - can only be renamed within the same directory -->
				<!-- if no renaming is required leave this empty or delete it -->
				<!-- one possible example of this pattern is myFile_${feedInputFileName}_${feedInputFileProcessingStartedTimestamp}.txt -->
				<output-file-name-pattern></output-file-name-pattern>
				<!-- executed by DB loading threads -->
				<afterBulkLoadSuccess>
					<!-- statements to be invoked after bulkLoadInsertStatement was successfully invoked. Thread that does bulk loading executes this! Commands are executed in order -->
					<command type="shell">d:/first.bat</command>
					<command type="sql">CALL simpleproc1(@a)</command>
					<command type="sql">CALL simpleproc2(@a)</command>
					<command type="shell">d:/second.bat</command>
				</afterBulkLoadSuccess>
				<onBulkLoadFailure>
					<!-- by default files are moved to error folder on processing failure -->
					<!-- statements to be invoked after loading of bulk data failed (regardless whether jdbc or file loading is used). Thread that does bulk loading executes this! Commands are executed in order -->
					<command type="shell">d:/first.bat</command>
					<command type="sql">CALL simpleproc1(@a)</command>
					<command type="sql">CALL simpleproc2(@a)</command>
					<command type="shell">d:/second.bat</command>
				</onBulkLoadFailure>
			</bulkLoadDefinition>
			<!-- following is executed by feed processing threads in the order specified -->
			<afterFeedProcessingCompletion>
				<command type="shell">d:/first.bat</command>
				<!-- it is possible to use any context attributes here -->
				<command type="sql">CALL warehouse.DATAFEED('${guid}', 'analyticsRequestGuid', '${runTag}', trunc(to_date('${businessDate}','DD/MM/YYYY')), '${rerunGuid}','requestName','batchType','${portfolio}', ${RWH_PORTFOLIO_DIM.sk}, ${completionNumberOfRowsInFeed},to_timestamp('${feedInputfileReceivedDateTime}', 'DD/MM/YYYY HH24:MI:SS.FF3'),
                      'CS', '${intradayName}', '${guid}','RISK', 'RISK_fileName', trunc(to_timestamp('${feedInputfileProcessingStartedDateTime}','DD/MM/YYYY HH24:MI:SS.FF3')), '${fileUserName}','${location}','${qlVersion}','${completionFeedProcessingSuccessFailureFlag}',
                      '${completionFeedProcessingErrorDescription}', '${eodFlag}', '${requestTimestamp}','${rerunVersion}')</command>
				<command type="shell">d:/second.bat</command>
			</afterFeedProcessingCompletion>
			<!-- following is executed by feed processing thread in the order specified, only if feed processing failed -->
			<onFeedProcessingFailure>
				<!-- by default files are moved to error folder on processing failure -->
				<command type="shell">d:/rollback.bat</command>
				<!-- it is possible to use any context attributes here -->
				<command type="sql">UPDATE some table for rollback</command>
				<command type="shell">d:/second_rollback.bat</command>
			</onFeedProcessingFailure>
			<threadPoolSizes>
				<!-- 0 to turn off feature, any other positive integer to turn it on and use that many threads (per feed) for processing -->
				<feedProcessingThreads>4</feedProcessingThreads>
				<bulkLoadProcessingThreads>0</bulkLoadProcessingThreads>
			</threadPoolSizes>
		</factFeed>
	</factFeeds>
	<dimensions>
		<!-- surrogate key for every dimension used in feed will be exposed as context attribute under name ${DIMENSION_NAME.sk}, for last line in feed only -->
		<!-- exposeLastLineValueInContext - whether surrogate key caculated for the last line of every feed should be available in context as attribute named ${DIMENSION_NAME.sk}, default is false -->
		<dimension name="RWH_UNDERLYING_DIM" type="T1_INSERT_ONLY" exposeLastLineValueInContext="false">
			<!-- how many elements can be stored in local cache for this dimension -->
			<!-- if not set then default value will be used -->
			<localCacheMaxSize>10000</localCacheMaxSize>
			<mappedColumns>
				<mappedColumn name="underlyingCode" naturalKey="true" />
				<mappedColumn name="riskUnderlyingCode" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingle>
					insert into rwh_underlying_dim(underlying_dim_wid, underlying_name, risk_underlying_name)
					values (rwh_underlying_seq.nextval,'${underlyingCode}','${riskUnderlyingCode}')
				</insertSingle>
				<!-- must return single value (surrogate key). Ideally WHERE clause should use all naturalKey columns from <mappedColumns> section -->
				<selectSurrogateKey>select underlying_dim_wid from rwh_underlying_dim where underlying_name='${underlyingCode}' and risk_underlying_name='${riskUnderlyingCode}'</selectSurrogateKey>
				<!--  order is important, first return surrogate key and then all natural keys, in order as defined by <mappedColumns> -->
				<preCacheKeys>select underlying_dim_wid, underlying_name, risk_underlying_name from rwh_underlying_dim</preCacheKeys>
			</sqlStatements>
		</dimension>
		<dimension name="RWH_RISK_STATUS_DIM" type="T1_INSERT_ONLY">
			<mappedColumns>
				<mappedColumn name="exceptionCode" naturalKey="true" />
				<mappedColumn name="exceptionDescription" naturalKey="true" />
				<mappedColumn name="exceptionFlag" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingle>
					insert into rwh_risk_status_dim(risk_status_dim_wid, status_code, status_description, status_flag)
    				values (rwh_risk_status_seq.nextval, '${exceptionCode}', '${exceptionDescription}', substr('${exceptionFlag}', 0, 1))
				</insertSingle>
				<selectSurrogateKey>select risk_status_dim_wid from rwh_risk_status_dim where status_code='${exceptionCode}' and status_description='${exceptionDescription}' and status_flag=substr('${exceptionFlag}', 0, 1)</selectSurrogateKey>
				<preCacheKeys>select risk_status_dim_wid, status_code, status_description, status_flag from rwh_risk_status_dim</preCacheKeys>
			</sqlStatements>
		</dimension>
		<dimension name="RWH_PRODUCT_DIM" type="T1_INSERT_ONLY">
			<mappedColumns>
				<mappedColumn name="productCode" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingle>
					insert into rwh_product_dim(product_dim_wid, product_code)
    				values (rwh_product_seq.nextval, '${productCode}')
				</insertSingle>
				<selectSurrogateKey>select product_dim_wid from rwh_product_dim where product_code='${productCode}'</selectSurrogateKey>
				<preCacheKeys>select product_dim_wid, product_code from rwh_product_dim</preCacheKeys>
			</sqlStatements>
		</dimension>
		<dimension name="RWH_PORTFOLIO_DIM" type="T1_INSERT_ONLY">
			<mappedColumns>
				<mappedColumn name="portfolio" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingle>
					insert into rwh_portfolio_dim(portfolio_dim_wid, portfolio_name)
    				values (rwh_portfolio_seq.nextval, '${portfolio}')
				</insertSingle>
				<selectSurrogateKey>select portfolio_dim_wid from rwh_portfolio_dim where portfolio_name='${portfolio}'</selectSurrogateKey>
				<preCacheKeys>select portfolio_dim_wid, portfolio_name from rwh_portfolio_dim</preCacheKeys>
			</sqlStatements>
		</dimension>
		<dimension name="RWH_PARTY_DIM" type="T1_INSERT_ONLY">
			<mappedColumns>
				<mappedColumn name="partyName" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingle>
					insert into rwh_party_dim(party_dim_wid, party_name)
    				values (rwh_party_seq.nextval, '${partyName}')
				</insertSingle>
				<selectSurrogateKey>select party_dim_wid from rwh_party_dim where party_name='${partyName}'</selectSurrogateKey>
				<preCacheKeys>select party_dim_wid, party_name from rwh_party_dim</preCacheKeys>
			</sqlStatements>
		</dimension>
		<dimension name="RWH_CURRENCY_DIM" type="T1_INSERT_ONLY">
			<mappedColumns>
				<mappedColumn name="baseCurrencyCode" naturalKey="true" />
				<mappedColumn name="riskCurrencyCode" naturalKey="true" />
				<mappedColumn name="localCurrencyCode" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingle>
					insert into rwh_currency_dim(CURRENCY_DIM_WID, BASE_CURRENCY, BASE_CURRENCY_DESC, RISK_CURRENCY, RISK_CURRENCY_DESC, LOCAL_CURRENCY, LOCAL_CURRENCY_DESC) 
					values (rwh_currency_seq.nextval, '${baseCurrencyCode}', '${baseCurrencyCode}', '${riskCurrencyCode}', '${riskCurrencyCode}', '${localCurrencyCode}', '${localCurrencyCode}')
				</insertSingle>
				<selectSurrogateKey>select CURRENCY_DIM_WID from rwh_currency_dim where BASE_CURRENCY='${baseCurrencyCode}' and RISK_CURRENCY='${riskCurrencyCode}' and LOCAL_CURRENCY='${localCurrencyCode}'</selectSurrogateKey>
				<preCacheKeys>select CURRENCY_DIM_WID, BASE_CURRENCY, RISK_CURRENCY, LOCAL_CURRENCY from rwh_currency_dim</preCacheKeys>
			</sqlStatements>
		</dimension>
		<dimension name="RWH_CURVETYPE_DIM" type="T1_INSERT_ONLY">
			<mappedColumns>
				<mappedColumn name="curveType" naturalKey="true" />
				<mappedColumn name="pricingModel" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingle>
					insert into rwh_curvetype_dim(curvetype_dim_wid, curve_type_code, curve_type_desc, pricing_model)
    				values (rwh_curve_type_seq.nextval, '${curveType}', '${curveType}', '${pricingModel}')
				</insertSingle>
				<selectSurrogateKey>select curvetype_dim_wid from rwh_curvetype_dim where curve_type_code='${curveType}' and pricing_model='${pricingModel}'</selectSurrogateKey>
				<preCacheKeys>select curvetype_dim_wid, curve_type_code, pricing_model from rwh_curvetype_dim</preCacheKeys>
			</sqlStatements>
		</dimension>
		<dimension name="RWH_DATE_DIM" type="T1_INSERT_ONLY">
			<mappedColumns>
				<mappedColumn name="businessDate" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingle>
					insert into WAREHOUSE.RWH_DATE_DIM(DATE_DIM_WID, BUSINESS_DATE)
					values (TO_NUMBER(TO_CHAR(TO_TIMESTAMP('${businessDate}', 'DD/MM/YYYY'), 'YYYYMMDD')), TO_TIMESTAMP('${businessDate}', 'DD/MM/YYYY'))
				</insertSingle>
				<selectSurrogateKey>select DATE_DIM_WID from RWH_DATE_DIM where BUSINESS_DATE=TO_TIMESTAMP('${businessDate}', 'DD/MM/YYYY')</selectSurrogateKey>
				<preCacheKeys>select DATE_DIM_WID, TO_CHAR(BUSINESS_DATE, 'DD/MM/YYYY') from RWH_DATE_DIM</preCacheKeys>
			</sqlStatements>
		</dimension>
		<dimension name="RWH_DEAL_DIM" type="T1_INSERT_ONLY">
			<mappedColumns>
				<!-- natural key columns are used to cache data -->
				<!-- if not natural key columns are defined caching will be disabled -->
				<!-- mapped columns are used to instruct engine what attributes from context (or feed) should be used to prepare statements before execution -->
				<!-- all mapped column names will be searched in statements (before execution) and replaced if found with values found in feed or context -->
				<mappedColumn name="sourceName" naturalKey="true" />
				<mappedColumn name="dealType" naturalKey="true" />
				<mappedColumn name="dealId" naturalKey="true" />
				<mappedColumn name="dealVersion" naturalKey="true" />
				<mappedColumn name="dealWhatIfUID" naturalKey="true" />
				<mappedColumn name="componentCode" naturalKey="true" />
				<mappedColumn name="trade_tds" />
				<mappedColumn name="dealAmended" />
				<mappedColumn name="structureCd" />
				<mappedColumn name="maturityDate" />
				<mappedColumn name="dealDate" />
				<mappedColumn name="dealState" />
			</mappedColumns>
			<sqlStatements>
				<insertSingle>
					insert into rwh_deal_dim(deal_dim_wid, sourceName, deal_type, deal_id, deal_version, deal_Whatif_uid, component_code,
       					deal_date, deal_amended, deal_state, deal_structure_CD, deal_additional_info, maturity_date, deal_additional_info_cube, dealInCubeGeneration)
    				values (rwh_deal_seq.nextval, '${sourceName}', '${dealType}', '${dealId}', ${dealVersion}, '${dealWhatIfUID}', '${componentCode}', TO_DATE('${dealDate}', 'DD/MM/YYYY HH24:MI:SS'), TO_DATE('${dealAmended}', 'DD/MM/YYYY HH24:MI:SS'),
    				'${dealState}', '${structureCd}', '${trade_tds}', TO_DATE('${maturityDate}', 'DD/MM/YYYY HH24:MI:SS'), '${trade_tds}', NULL)
				</insertSingle>
				<selectSurrogateKey>select deal_dim_wid from rwh_deal_dim where sourceName='${sourceName}' and deal_type='${dealType}' and deal_id='${dealId}' and deal_version='${dealVersion}' and DEAL_WHATIF_UID='${dealWhatIfUID}' and component_code='${componentCode}'</selectSurrogateKey>
				<preCacheKeys>select deal_dim_wid, sourceName, deal_type, deal_id, deal_version, DEAL_WHATIF_UID, component_code from rwh_deal_dim</preCacheKeys>
			</sqlStatements>
		</dimension>
		<!-- this dimension will be executed once per feed (before starting to process feed file) and its key will be available as ${notificationSk} in context for feed processing (not available in context for bulk processing)-->
		<!-- this is normal dimension, caching still applies - the only difference from other dimension is that SK is looked up once per feed and not once per feed row -->
		<dimension name="REQUESTNOTIFICATION" type="T1_INSERT_ONLY" cacheKeyPerFeedInto="notificationSk">
			<mappedColumns>
				<!-- ffn[x] attributes are created by default feed file name processor. Delimiter for splitting feed file name is _ -->
				<!-- so if input feed file is named a_b_c then ffn[0]=a ffn[1]=b ffn[2]=c and can be used as any other attribute in context -->
				<!-- this can be changed by providing custom FeedFileNameProcessor implementation (see customisations folder for example) -->
				<mappedColumn name="ffn[1]" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingle>
					insert into REQUESTNOTIFICATION (NOTIFICATIONID,PERSISTSTARTTIMESTAMP,PERSISTSTATUS,UNIQUENAME,ANALYTICSREQUESTGUID, SOURCE, SOME_NUMBER) 
					values (rwh_request_notification_seq.nextval, sysdate,'R', '${ffn[1]}', '${ffn[1]}', substr('${feedInputFileName}',1,7), ${feedProcessingThreadID})
				</insertSingle>
				<selectSurrogateKey>select NOTIFICATIONID from REQUESTNOTIFICATION where UNIQUENAME='${ffn[1]}'</selectSurrogateKey>
				<preCacheKeys>select NOTIFICATIONID, UNIQUENAME from REQUESTNOTIFICATION</preCacheKeys>
			</sqlStatements>
		</dimension>
	</dimensions>
</config>
