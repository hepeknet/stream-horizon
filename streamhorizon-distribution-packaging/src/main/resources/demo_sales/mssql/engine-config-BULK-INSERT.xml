<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<config>	
	<connectionProperties>
		<jdbcUrl><!--SET ME! please change server name (instead of localhost) and port if required, final connection string look like be: jdbc:sqlserver://localhost:1433;user=sh;password=StreamHorizon369;databaseName=sh;PacketSize=-1;protocol=ncalrpc;TransactionMode=implicit;applicationName=StreamHorizon;--></jdbcUrl>	
		<jdbcPoolSize>100</jdbcPoolSize>
	</connectionProperties>
	<feeds>	
		<feed name="myDataFeed1" type="full"> 		
			<source type="file">
				<properties>
					<property name="directoryPath"><!--SET ME! your file feed directory comes here, use /dir1/dir2 format for unix and c:/dir1/dir2 format for windows OS setup--></property>
					<property name="fileNameMask">.*csv</property>
				</properties>
			</source>
			<archiveDirectory><!--SET ME! your archive directory comes here, use /dir1/dir2 format for unix and c:/dir1/dir2 format for windows OS setup--></archiveDirectory>
			<errorDirectory><!--SET ME! your error directory comes here, use /dir1/dir2 format for unix and c:/dir1/dir2 format for windows OS setup--></errorDirectory>			
			<threadPoolSettings>
				<etlProcessingThreadCount><!--SET ME! set number of etl threads (etl parallelism). ideal setting will achieve maximal throughput to target database. to start with set this number to number of cores of your server. try reducing/increasing number depending on CPU% utilisation of your server--></etlProcessingThreadCount>	
				<databaseProcessingThreadCount><!--SET ME! set number of db loader threads (db load parallelism). ideal setting will achieve maximum throughput to target database. to start with set this number to 40 which is maximum paralelism as sales_fact table has 40 partitions. depending on I/O performance paralelism of 40 may be too high (or too low), in which case trying setting less/more than 40 could be benefitial for throughput. if you need higher paralelism (40+) please alter create table (data file, log file and partition scheme) scripts for all sales_fact table --></databaseProcessingThreadCount>
			</threadPoolSettings>					
			<sourceFormatDefinition>
				<delimiterString>,</delimiterString>
				<header process="normal">
					<eachLineStartsWithCharacter>1</eachLineStartsWithCharacter>
					<attributes>
						<attribute name="bookingDate" />
					</attributes>
				</header>
				<data process="no_validation">
					<attributes>
						<attribute name="productName" />
						<attribute name="productModel" />
						<attribute name="productCategory" />
						<attribute name="productCost" />
						<attribute name="customerName" />
						<attribute name="customerAddress" />
						<attribute name="customerCountry" />
						<attribute name="customerPhone" />
						<attribute name="employeeNumber" />
						<attribute name="employeeName" />
						<attribute name="salesChannelName" />
						<attribute name="promotionName" />
						<attribute name="discountPCT" />
						<attribute name="shipmentDate" />
						<attribute name="salesDate" />
						<attribute name="deliveryDate" />
						<attribute name="priceBeforeDiscount" />
						<attribute name="priceAfterDiscount" />
						<attribute name="saleCosts" />
						<attribute name="supplierName" />
					</attributes>
				</data>
			</sourceFormatDefinition>
			<target type="file">
				<properties>  
					<property name="bulkOutputDirectory"><!--SET ME! your bulk directory comes here(default location is $ENGINE_HOME/demo_sales/mssql/bulk_loader.bat), use /dir1/dir2 format for unix and c:/dir1/dir2 format for windows OS setup--></property>
					<property name="bulkLoadOutputExtension">data</property>
				</properties>							
				<bulkLoadInsert>
					<command type="shell"><!--SET ME! set full file path to the bulk_loader.bat file and leave '${bulkFileName}' as parameter to the bat exectuable. example: D:\bin\bulk_loader.bat ${bulkFileName} --></command>			
				</bulkLoadInsert>	
			</target>		
			<targetFormatDefinition>
				<attributes>
					<attribute name="dimension.employee_dim" type="int" />				
					<attribute name="dimension.customer_dim" type="int" />
					<attribute name="dimension.product_dim" type="int" />
					<attribute name="dimension.sales_channel_dim" type="int" />
					<attribute name="dimension.promotion_dim" type="int" />						
					<attribute name="dimension.supplier_dim" type="int" />
					<attribute name="bookingDate"  type="int" />
					<attribute name="feed.salesDate"  type="int" />
					<attribute name="feed.deliveryDate"  type="int" />
					<attribute name="feed.priceBeforeDiscount" type="float" />
					<attribute name="feed.priceAfterDiscount" type="float" />
					<attribute name="feed.saleCosts" type="float" />
					<attribute name="feedProcessingThreadID" type="int" />
					<attribute type="string">-1</attribute>
				</attributes>
			</targetFormatDefinition>				
			<events>					
			  	<onStartupCommands>
					<command type="sql">truncate table sales_fact</command>				
					<command type="sql">truncate table sh_metrics</command>
				</onStartupCommands> 
				<afterBulkLoadCompletion>
					<command type="sql">
						EXEC log_sh_metrics_bulk 'server_1_mssql',${engineInstanceIdentifier},${engineInstanceStartTimestamp},'afterBulkLoadCompletion','${bulkFileName}' ,${bulkFileProcessingStartedTimestamp},${bulkFileProcessingFinishedTimestamp}, '${bulkCompletionProcessingSuccessFailureFlag}', '${bulkCompletionProcessingErrorDescription}',${bulkProcessingThreadID}, '${bulkFilePath}'
					</command>
				</afterBulkLoadCompletion>		
				<afterFeedProcessingCompletion>	
					<command type="sql">
						EXEC log_sh_metrics 'server_1_mssql',${engineInstanceIdentifier},${engineInstanceStartTimestamp},'afterFeedProcessingCompletion',${feedInputFileReceivedTimestamp},${feedProcessingThreadID},'${feedInputFileName}',${feedInputFileProcessingStartedTimestamp},${feedInputFileProcessingFinishedTimestamp},${feedInputFileJdbcInsertStartedTimestamp},${feedInputFileJdbcInsertFinishedTimestamp},'${bulkFileAlreadySubmittedForLoading}',${bulkProcessingThreadID},'${bulkFilePath}','${bulkFileName}',${feedCompletionNumberOfTotalRowsInFeed},${bulkFileReceivedForProcessingTimestamp},${bulkFileProcessingStartedTimestamp},${bulkFileProcessingFinishedTimestamp},'${feedCompletionProcessingSuccessFailureFlag}','${feedCompletionProcessingErrorDescription}'
					</command>						
				</afterFeedProcessingCompletion>				
			</events>				
		</feed>
	</feeds>
	<dimensions>
		<dimension name="employee_dim" type="INSERT_ONLY">		
			<mappedColumns>
				<mappedColumn name="employeeName" naturalKey="true" />
				<mappedColumn name="employeeNumber" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingleRecord>
					insert into employee_dim(employee_name, employee_number) values ('${employeeName}', ${employeeNumber})
				</insertSingleRecord>
				<selectRecordIdentifier>select employee_id from employee_dim where employee_name='${employeeName}' and employee_number=${employeeNumber}</selectRecordIdentifier>
				<preCacheRecords>select employee_id, employee_name, employee_number from employee_dim</preCacheRecords>
			</sqlStatements>
		</dimension>
		<dimension name="customer_dim" type="INSERT_ONLY">		
			<mappedColumns>
				<mappedColumn name="customerName" naturalKey="true" />
				<mappedColumn name="customerAddress" naturalKey="true" />
				<mappedColumn name="customerCountry" naturalKey="true" />
				<mappedColumn name="customerPhone" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingleRecord>
					insert into customer_dim(customer_name, customer_address, customer_country, customer_phone) values ('${customerName}','${customerAddress}','${customerCountry}','${customerPhone}')
				</insertSingleRecord>
				<selectRecordIdentifier>select customer_id from customer_dim where customer_name='${customerName}' and customer_address='${customerAddress}' and customer_country='${customerCountry}' and customer_phone='${customerPhone}'</selectRecordIdentifier>
				<preCacheRecords>select customer_id, customer_name, customer_address, customer_country, customer_phone from customer_dim</preCacheRecords>
			</sqlStatements>
		</dimension>
		
		<dimension name="product_dim" type="INSERT_ONLY">		
			<mappedColumns>
				<mappedColumn name="productName" naturalKey="true" />
				<mappedColumn name="productModel" naturalKey="true" />
				<mappedColumn name="productCategory" naturalKey="true" />
				<mappedColumn name="productCost" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingleRecord>
					insert into product_dim(product_name, product_model, product_category, product_cost) values ('${productName}', '${productModel}', '${productCategory}', '${productCost}')
				</insertSingleRecord>
				<selectRecordIdentifier>select product_id from product_dim where product_name='${productName}' and product_model='${productModel}' and product_category='${productCategory}' and product_cost='${productCost}'</selectRecordIdentifier>
				<preCacheRecords>select product_id, product_name, product_model, product_category, product_cost from product_dim</preCacheRecords>
			</sqlStatements>
		</dimension>		
		<dimension name="supplier_dim" type="INSERT_ONLY">		
			<mappedColumns>
				<mappedColumn name="supplierName" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingleRecord>
					insert into supplier_dim(supplier_name) values ('${supplierName}')
				</insertSingleRecord>
				<selectRecordIdentifier>select supplier_id from supplier_dim where supplier_name='${supplierName}'</selectRecordIdentifier>
				<preCacheRecords>select supplier_id, supplier_name from supplier_dim</preCacheRecords>
			</sqlStatements>
		</dimension>
		<dimension name="sales_channel_dim" type="INSERT_ONLY">		
			<mappedColumns>
				<mappedColumn name="salesChannelName" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingleRecord>
					insert into sales_channel_dim(sales_channel_name) values ('${salesChannelName}')
				</insertSingleRecord>
				<selectRecordIdentifier>select sales_channel_id from sales_channel_dim where sales_channel_name='${salesChannelName}'</selectRecordIdentifier>
				<preCacheRecords>select sales_channel_id, sales_channel_name from sales_channel_dim</preCacheRecords>
			</sqlStatements>
		</dimension>
		<dimension name="promotion_dim" type="INSERT_ONLY">		
			<mappedColumns>
				<mappedColumn name="promotionName" naturalKey="true" />
				<mappedColumn name="discountPCT" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingleRecord>
					insert into promotion_dim(promotion_name, discount_pct) values ('${promotionName}', ${discountPCT})
				</insertSingleRecord>
				<selectRecordIdentifier>select promotion_id from promotion_dim where promotion_name='${promotionName}' and discount_pct=${discountPCT}</selectRecordIdentifier>
				<preCacheRecords>select promotion_id, promotion_name, discount_pct from promotion_dim</preCacheRecords>
			</sqlStatements>
		</dimension>
	</dimensions>
	<properties>	
		<property name="jdbc.bulk.loading.batch.size">2000</property>
		<property name="output.processing.statistics">true</property>
	</properties>	
</config>