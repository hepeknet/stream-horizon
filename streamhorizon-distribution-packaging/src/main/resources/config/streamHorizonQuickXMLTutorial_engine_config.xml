<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<config>
	<!-- RDBMS JDBC connection properties -->
	<connectionProperties>	
		<jdbcUrl>jdbc:oracle:thin:@192.168.0.14:1521:orcl</jdbcUrl>
		<jdbcUserName>warehouse</jdbcUserName>
		<jdbcPassword>warehouse</jdbcPassword>
		<!-- jdbcPoolSize should be calculated as: numberOfDimensions + number of parallel jdbc connections + contingency of 10 connections. if you have 10 dimensions in your model and 50 etl threads <feedProcessingThreads> or 50 DB threads <bulkLoadProcessingThreads> inserting into fact table connection pool should have minimum of 10+50 dimensions. for contingency set this parameter to 10+50+10. it is important to avoid reusing of jdbc connections by connection pool of fact loader processes as this can be detrimental for performance. default setting is 25. -->
		<jdbcPoolSize>100</jdbcPoolSize>
	</connectionProperties>
	<!-- Character used to enclose string values. Usually this is single quote. Check your database manual -->
	<databaseStringLiteral>'</databaseStringLiteral>
	<!-- Character used to escape databaseStringLiteral (see above). Usually this is double quote. Check your database manual-->
	<databaseStringEscapeLiteral>''</databaseStringEscapeLiteral>
	<feeds>
		<!-- setup one <feed> element for every file format (message, database table or any data source)  which engine needs to process. <feed> name must be unique within xml definition. please use 'full' type of feed as a default unless your feed meets specific delta, repetitive or control criteria-->	
		<!-- feed can be of type full, repetitive, delta and control -->
		<!-- control feed can not have header and footer and it can have only one line, which will be exposed as context attributes -->
		<!-- for "delta" feed it is mandatory to set nullString value, see below 
		     for "repetitive feed" you can set repetition count element to number of attributes to be skipped while processing:
		<repetitionCount>10</repetitionCount>
		-->
		<feed name="MyDataFeed1" type="full">
			
			<!-- if they do not exist directories will be created by engine -->				

			<!-- type of source can be file, rpc, jdbc -->
			<source type="file">
				<properties>
					<!-- source directory is the directory from which file feeds will be processed -->
					<property name="directoryPath">/tmp/stream_horizon/input</property>
					<!-- supply fileName masks for the fees to which this <feed> format definition is applicable. feeds will be loaded from source directory. Multiple masks can be specified 					
						
					When fileNameMask property is set to for example *.gz or *.zip engine will decompress files on the fly-->
					<property name="fileNameMask">.*FEED.*done</property>
				</properties>
			</source>
			
			<!--
			
			When using JDBC as feed source.
			
			<source type="jdbc">
				<properties>
					// statement to use to select feed data. It must return data format as it is described in sourceFormatDefinition. This means that number of attributes (selected columns of sql query) of "sqlStatement" and number of attributed defined in <sourceFormatDefinition><data><attributes> element must match.
					<property name="sqlStatement">select column1, column2, column3, column4 from yourSourceTable order by id</property>
					// how often to run previous statement? any CRON expression supported by quartz is applicable. For more options see http://quartz-scheduler.org/
					<property name="schedule">0/2 * * * * ?</property>
					// JDBC connection string where source (feed) data can be found. Any JDBC data source is supported
					<property name="jdbcUrl">jdbc:h2:mem:stream_horizon_test;DB_CLOSE_DELAY=-1</property>
				</properties>
			</source>
			-->
			
			<!-- 
			
			When using RPC (Thrift) as a feed source. Thrift file is available in streamhorizon-api-X.Y.Z.jar and you should use it to generate your client code.
			Then you can connect to Thrift server started by StreamHorizon engine (see configuration below).
			
			<source type="rpc">
				<properties>
					// port on which Thrift server started as part of StreamHorizon engine should listen to
					<property name="port">7890</property>
				</properties>
			</source>
			 -->
			
			<!-- archive directory is directory into which successfully processed feeds will be moved-->
			<archiveDirectory>/tmp/stream_horizon/archive</archiveDirectory>
			<!-- error directory will contain feeds from source folder which if processing fails for any reason. for error & logging information please look at $ENGINE_HOME/log/engine*log -->
			<errorDirectory>/tmp/stream_horizon/error</errorDirectory>
			
			<!--engine has two threadpools:
				1) etl threads (members of etl threadpool) <etlProcessingThreadCount> either:
					a) process feeds from source directory into bulk directory (bulk files are later on picked up by DB threads from DB threadpool)
					b) process feeds from source directory and insert them directly into database via jdbc without creating bulk files. if using jdbc inserts into fact table set DB threadpool threadcount to zero <databaseProcessingThreadCount>.
				2) db threads (members of db threadpool) <databaseProcessingThreadCount> are used to bulk load via BULK INSERT or SQL*Loader or External table or procedure (functions aren't supported) call (which invokes external table definition or equivalent).
			-->
			<threadPoolSettings>
				<!-- 0 to turn off feature, any other positive integer to turn it on and use that many threads (per feed) for processing -->
				<!-- set number of etl threads. etl threads process feeds and create bulk files (or execute jd<bc batched statements) -->
				<etlProcessingThreadCount>20</etlProcessingThreadCount>
				<!--set number of db threads which will concurrently load bulk files into the fact table. If 0 no bulk loading will happen -->
				<databaseProcessingThreadCount>0</databaseProcessingThreadCount>
			</threadPoolSettings>				
			
			<sourceFormatDefinition>
				<!-- <nullString> is used only by 'type="delta"' (not 'type="full"') feed parsers -->
				<nullString>!</nullString>
				<!-- Class name of custom implementation for feed file name processor. In case when some information should be extracted from file name of every input feed file it is possible to provide customized implementation to parse file name and provide new context attributes. Check CustomFeedFileNameProcessor.java in $ENGINE_HOME/plugins/ folder for example -->
				<!--
				<fileNameProcessorClassName>CustomFeedFileNameProcessor</fileNameProcessorClassName>
				-->
				<!-- <delimiterString> defnes delimiter used in your feed -->
				<delimiterString>^$</delimiterString>
				<!---->
				<!--CONTEXT:  every header, footer, body or custom attribute you define in your feed can be referenced from context of ETL engine. context attributes can be used to generate select or insert statements against database or can be used for generation of bulk files.
				
				note: Engine generates and writes out at console 'Implicitly available attributes' context attributes at startup. These attributes can be used for logging and metrics. attribute bulkProcessingThreadID or feedProcessingThreadID which represent etl thread ID and db thread id (<eedProcessingThreads> and <bulkLoadProcessingThreads> respectively) can be used to achieve simultaneous partition/subpartition loading and avoid forming of locks. Please see console at startup for list of all available attributes.
											
				-->
				<!-- define file header structure. if feed has no header set this value to no_header. If there is header and it should not be processed set this value to skip 
					Note that if your feed has attribute which is repeated for every single data record it is ideal candidate to become header attribute -->
				<header process="normal">
					<!--you can utilize custom header record parser class if you need to calculate/define new context parameter based on some of the header or Implicitly available engine attributes or execute any other custom logic. note that this may significantly slow down the engine! java extension will be invoked once for every header record (once per input feed file)-->
					<!-- check CustomHeaderParser.java in $ENGINE_HOME/plugins/ folder for example -->
					<!--
					<headerParserClassName>CustomHeaderParser</headerParserClassName>
					-->
					<!--if header line is defined as line with special character please specify character here. it is assumed that character is delimited from header attributes by <delimiterString>. if header line has no special character erase this element -->
					<eachLineStartsWithCharacter>H</eachLineStartsWithCharacter>
					<!--define context names for all data attributes (in order as they appear in your data source)-->
					<attributes>
						<attribute name="headerAttributeName_1" />
						<attribute name="headerAttributeName_2" />
						<attribute name="headerAttributeName_3" />
					</attributes>
				</header>
				<!--define data (body) feed record structure-->
				<!-- process attribute can have values normal (first character of every data line will be checked), no_validation (first character of every data line will not be checked) -->
				<data process="normal">			
					<!--if data line is defined as line with special character please specify character here and set process='normal'. It is assumed that character is delimited from data attributes by <delimiterString>. if data line has no special character erase this element and set process='no_validation' -->			
					<eachLineStartsWithCharacter>D</eachLineStartsWithCharacter>
					<!--you can utilize custom data record parser class if you need to calculate/define new context parameter based on some of the header or Implicitly available engine attributes or execute any other custom logic. note that this may significantly slow down the engine! java extension will be invoked once for every data record (ever data row in every input feed file)-->
					<!-- check CustomDataMapper.java in $ENGINE_HOME/plugins/ folder for example -->				
					<!--
					<feedDataProcessorClassName>CustomDataMapper</feedDataProcessorClassName>
					-->
					<!--define context names for all data attributes (in order as they appear in your data source)-->				
					<attributes>
						<attribute name="dataAttributeName_1" />
						<attribute name="dataAttributeName_2" />
						<attribute name="dataAttributeName_3" />
						<attribute name="dataAttributeName_4" />
						<attribute name="dataAttributeName_5" />
						<attribute name="dataAttributeName_6" />
						<attribute name="dataAttributeName_7" />
					</attributes>
				</data>
				<!--define footer feed record structure-->
				<!-- significance of footer is to hold recordCount (number of expected records within data file and compare them to actual number of processed records from the feed file). if those two don't match engine will fail processing -->
				<footer process="strict">
					<!--if footer line is defined as line with special character please specify character here. it is assumed that character is delimited from data attributes by <delimiterString>. if footer line has no special character erase this element -->	
					<eachLineStartsWithCharacter>F</eachLineStartsWithCharacter>
					<!-- at what position will footer 'recordCount' attribute be found. if recordCount is first attribute in the footer set 1, if N'th attribute in the footer set property to N. note that delimiter for footer is read from <delimiterString> element -->
					<recordCountAttributePosition>1</recordCountAttributePosition>
				</footer>
			</sourceFormatDefinition>
			<!-- Target type can have one of the following values: file/none/zip/gz/jdbc/none 
				
				When set to 'none' if you want to test performance of processing without actually writing bulk file to bulk output directory. 'none' value tests performance by eliminating write I/O 
				
				When set to 'zip' output file will be zip compressed and will have .zip extension added
				
				When set to 'gz' output file will be gzip compressed and will have .gz extension added
				
				When set to 'jdbc' you must specify types for all attributes in <targetFormatDefinition> element and bulk records will be persisted using JDBC bulk interface
				
				When set to 'pipe' ETL threads use named pipes to transfer data. Have to follow same guidelines as 'file' 
				
				It is also possible to define your own target types by providing plugin. See for example HDFS target type in $ENGINE_HOME/plugins/HDFSOutputWriter.java
				
			-->						
			<target type="file">
					<properties>  
						<!-- Property "bulkOutputDirectory" names directory where bulk files will be created. Bulk files usually have format of a fact table and are inserted into database by use of SQL*Loader, External Tables in case of Oracle or BULK INSERT command for MSSQL-->
						<property name="bulkOutputDirectory">/tmp/stream_horizon/bulkOutput</property>
						<!-- Property "bulkLoadOutputExtension" defines the extension of bulk output file to be loaded by DB threads. DB threads will consider only files with given extension for loading into database. For example, if you expect 'zip' files to be loaded change this extension to zip (there is no need to add dot character for example '.zip'). If property is left as null or empty, extension will not be used as part of matching files in the bulk load folder -->							
						<property name="bulkLoadOutputExtension"></property> 
						<!-- Specifies delimiter to be used for attribute separation when creating bulk output file. Whenever possible use single character delimiter -->
						<property name="bulkLoadFileDelimiter">,</property> 
						<!-- Property "outputFileNamePattern" enforces renaming bulk file after it has been generated by the engine. If you need this functionality:
								-Set this parameter if bulk output file is to be renamed after it is created (but before loaded by DB threads)
								-Any value from context can be used as ${} placeholder and engine will replace it before renaming 
								-Renamed file will be left in <bulkOutputDirectory> - you can not move file to other folder using this functionality
								-If no renaming is required leave this element empty or remove it
								-One possible example of this pattern is myFile_${feedInputFileName}_${feedInputFileProcessingStartedTimestamp}.txt 
						-->
						<property name="outputFileNamePattern"></property>								
					</properties>							
					<!-- sql statement for loading bulk output file. Can be any sql statement, function call or stored procedure invocation -->
					<!-- in case of jdbc loading this statement must be prepared statement listing all attributes that are defined in bulkLoadFormatDefinition section -->	
						<!--EXAMPLES:-->				
						<!-- EXAMPLE 1) function call example:
							 this example shows how function can be called via jdbc. note that implicitly defined engine context parameter 'bulkProcessingThreadID' is used in order to loadbalance subpartitions of target fact table and avoid locking. also note that another implicit engine context parameter 'bulkFileName' is used as function internally need to alter table and repoint external table to data file which need be loaded. 
							 
							 prefixing context attributes with ${contextAttributeName} will ensure that engine engine replaces them before issuing the call against RDBMS
							 
							 This is most performant way to insert data into the fact table. note that best results have been achieved when bulk files aren't zipped or gz'ed and when parallelism is high (db threads load into 30-50 subpartitions. no logging mode will usually increase performance by 50%.
						-->
						<bulkLoadInsert>
							<command type="sql">
							CALL schema.functionName(mod(${bulkProcessingThreadID},50),${bulkProcessingThreadID}, '${bulkFileName}' )  
							</command>
						</bulkLoadInsert>	
						<!--EXAMPLE 2) direct jdbc insert example
							for performance reasons 'customized.formattedHeaderDate' customised attribute is used to deliver value which will denote table partition name P_${customized.formattedHeaderDate}. to test you may supply hardcoded value to start with P_200110101
							
							in similar fashion, in order to target right subpartition and enable parallel load of subpartitions, another context custom defined attribute customized.feedProcessingThreadID.modulo is used SP_${customized.feedProcessingThreadID.modulo}
						
							note that insert statement below has 7 question marks and 9 fact table attributes. Engine will replace all context attributes prior to issuing statement against the database. as context attributes ${*} will be replaced with concrete values <bulkLoadFormatDefinition> element will require definition for only 7 <attributes> elements for fact table. 7 defined attributes will be assigned in listed order to placeholder values (replaced with ? sign of sql query).
						-->					
						<!--bulkLoadInsert>
						<command type="sql">
							insert /*+ append_values */ into schema.factTableName 
							subpartition (P_${customized.formattedHeaderDate}_SP_${customized.feedProcessingThreadID.modulo}) 
							( tableAttribute1,  tableAttribute2, tableAttribute3,  tableAttribute4,  tableAttribute5, tableAttribute6,  tableAttribute7,tableAttribute8,  tableAttribute9)
							VALUES (?,?, ${customized.formattedHeaderDate} , ${customized.feedProcessingThreadID.modulo} ,?,?,?,?,?)
							</command>
						</bulkLoadInsert-->	
							<!--EXAMPLE 3) calling  MySQL external script or command line to bulk load the file:
						
						note that implicit Engine context attributes bulkFilePath and bulkProcessingThreadID are embedded & swapped to the command prior to its execution
						
						note that type of command is 'shell' rather than 'sql'
						-->
					<!--bulkLoadInsert>
					<command type="shell">
						LOAD DATA INFILE '${bulkFilePath}', ${bulkProcessingThreadID} INTO TABLE test_fact FIELDS TERMINATED BY '@@' LINES TERMINATED BY '\n'
						</command>
					</bulkLoadInsert-->	
					
						<!--EXAMPLE 4) calling  MSSQL external script or command line to bulk load a bulk file:
						
						note taht implicit Engine context attributes bulkFileName is embeded & swapped to the command prior to its execution
						
							note that type of command is 'shell' rather than 'sql'
						
						bulkLoadScript.bat should (after variable substitution) execute command similar to:
						CALL SQLCMD -S server\instance -q "BULK INSERT user.[dbo].[factTableName]   FROM 'pathAndFileNameToBeLoaded' WITH (FIELDTERMINATOR =',',ROWTERMINATOR ='0x0a',TABLOCK)"
							-->
					<!--bulkLoadInsert>
						<command type="shell">g:/bulkLoadScript.bat</command>
					</bulkLoadInsert-->					
			</target>				
			
			
			<targetFormatDefinition>
				<attributes>
					<!--
						Attributes must be supplied in order in which they will appear in the output. Target output is generated by SH context parameters in following ways:
						1) If you wish to directly copy attribute from your source of data (for example source file or relational table)  into the output you should use name of context attribute prefixed with 'feed.' . this approach is most commonly used for measures in the source feed as they usually don’t get transformed by ETL. Please see example below
						2) If you wish to use surrogate key (SK) of dimensional cache (which is most often the case) use name of dimension in question prefixed with 'dimension.' prefix. Please see example below.
						3) Output attributes can be hardcoded values (constants) see example below with 'Y',-1 and ‘N’. If using hardcoded constant as string make sure you put appropriate database string delimiters (single quotes in this example)
						4) Output attribute can be mapped to any implicit context (or custom context) attribute (just use context attribute name, no need for ${} characters.
					-->
					<!-- order of attributes is important and determines definition of created bulk file (or generated jdbc statement) -->
					<!-- dimension.XYZ means output calculated surrogateKey (SK or ID) for dimension XYZ (based on dimension definition (please see below)) -->
						<!-- datatypes supported are can be int/string/float -->
						<attribute name="dimension.dimension1" type="int" />					
						<attribute name="dimension.dimension2" type="int" />
					<!-- hardcoded/constant attributes. will be copied directly to output. if string don't forget to supply string delimiters -->
						<attribute type="int">-1</attribute>
						<attribute type="string">Y</attribute>
						<!-- copy directly from input feed, as is. 
					this is an example of mapping value of measure directly from the feed -->
						<attribute name="feed.dataAttributeName_7" type="float" />
						<!-- attribute value will be mapped from context. It can be implicit context attribute or custom -->
						<attribute name="customized.attribute.from.context" type="int" />
				</attributes>
			</targetFormatDefinition>	
			
			<events>		
			<!-- All commands for a given event are executed in order in which they are supplied in the configuration file.
				 All command statements of all events can utilize and will substitute values for any context attributes supplied (if any). Please see examples below. -->			
				<!-- at startup, prior to starting any processing engine will execute (in defined order) sql or script (shell) commands. please reference all recovery and startup related scripts in this section -->
				<onStartupCommands>
					<!-- all context attributes will be replaced in both sql and shell commands, use normal ${contextAttributeName} notation-->
					<command type="shell">d:/${someAttribute1}.bat</command>
					<command type="sql">update fact_table set key=${someAttribute} where key=-100</command>
					<command type="shell">d:/second.bat</command>
				</onStartupCommands>
				<afterBulkLoadSuccess>
					<!-- afterBulkLoadSuccess event is invoked after <bulkLoadInsert> statement is successfully executed. DB thread which does bulk loading executes supplied command(s) (if supplied) of this event. Commands are executed in given order and can receive/utilize any of engine context parameters. Any context attribute will be replaced with its value
					
					Note that Oracle uses CALL while MSSQL uses EXEC keyword for execution. other vendors may have different keywords-->
					<command type="shell">d:/first.bat ${someContextAttributePassedAsParameter}</command>
					<command type="sql">CALL simpleproc1(${someContextAttribute1})</command>
					<command type="sql">CALL simpleproc2('${firstContextAttr}',${secondContextAttr})</command>
					<command type="shell">d:/second.bat</command>
				</afterBulkLoadSuccess>
				<afterFeedSuccess>
					<!--afterFeedSuccess event is invoked upon successful execution of ETL thread fore ach data entity. Commands are executed in given order and can receive/utilize any of engine context parameters. Any context attribute will be replaced with its value
Note that Oracle uses CALL while MSSQL uses EXEC keyword for execution. other vendors may have different keywords-->
					<command type="shell">d:/first.bat ${someContextAttributePassedAsParameter}</command>
					<command type="sql">CALL simpleproc1(${someContextAttribute1})</command>
					<command type="sql">CALL simpleproc2('${firstContextAttr}',${secondContextAttr})</command>
					<command type="shell">d:/second.bat</command>
				</afterFeedSuccess>
				<afterBulkLoadFailure>
					<!-- afterBulkLoadFailure event will be invoked only after loading of bulk data file by DB trhread has failed (else it will be ommited). DB thread which does bulk loading executes given command(s) (if supplied). Commands are executed in given order and can receive/utilize any of engine context parameters.
										
					By default feeds (files) are moved to error directory (<errorDirectory>) on processing failure.
					
					Note that Oracle uses CALL while MSSQL uses EXEC keyword for execution. other vendors may have different keywords-->
					<command type="shell">d:/first.bat</command>
					<command type="sql">CALL simpleproc1(${someContextAttribute1})</command>
					<command type="sql">CALL simpleproc2('${firstContextAttr}',${secondContextAttr})</command>
					<command type="shell">d:/second.bat</command>
				</afterBulkLoadFailure>
				<afterBulkLoadCompletion>
					<!-- afterBulkLoadCompletion event is executed by DB threads in order specified. Event is executed irrespective of sucess/failure of DB thread processing (think of event as finally{} block equivalent in Java) -->				
					<command type="sql">INSERT INTO BULK_LOAD_REC(cnt,filepath) VALUES(555, '${bulkFilePath}')</command>
					<command type="shell">d:/first.bat</command>
				</afterBulkLoadCompletion>
				<beforeFeedProcessing>  
					<!-- beforeFeedProcessing event is executed by ETL threads (feed processing thread) before processing of feed starts -->
					<command type="sql">update something in database</command>
					<command type="shell">/opt/prepare.sh</command>                                          
				</beforeFeedProcessing>
				<beforeBulkLoadProcessing>
					<!-- beforeBulkLoadProcessingevent is executed by DB threads (db processing thread) before processing of data starts -->
					<command type="sql">update something in database</command>
					<command type="shell">/opt/prepare.sh</command>                                          
				</beforeBulkLoadProcessing>
				<afterFeedProcessingFailure>
					<!-- afterFeedProcessingFailure event is executed by ETL threads (feed processing threads) in the order specified in configuration file. Only if feed processing has failed this event will be triggered and supplied commands will be executed. Think of this event as catch{} block equivalent in Java
					
					By default files are moved to error directory (<errorDirectory>) on processing failure -->
					<command type="shell">d:/rollback.bat</command>
					<command type="sql">UPDATE some table for rollback</command>
					<command type="shell">d:/second_rollback.bat</command>
				</afterFeedProcessingFailure>			
				<afterFeedProcessingCompletion>
					<!-- afterFeedProcessingCompletion event is executed by ETL threads (feed processing threads) in specified order. Command(s) are is executed irrespective of sucess/failure of ETL thread processing (think of this as finally{} block equivalent in Java) -->	
					<command type="shell">d:/first.bat</command>
					<command type="sql">CALL schema.function('${dataAttributeName_1}', 
															 trunc(to_date('${dataAttributeName_3}','DD/MM/YYYY')), 
															 'hardcodedString',
															 ${dimension1.sk}, /* example of use of surrogateKey value from dimensional cache */
															 to_timestamp('${feedInputfileReceivedDateTime}', 'DD/MM/YYYY HH24:MI:SS.FF3'))
					</command>
					<command type="shell">d:/second.bat</command>
				</afterFeedProcessingCompletion>
			<events>
		</feed>
	</feeds>
	<dimensions>
		<!--Dimension definition enables caches (global cache (Infinispan) and local etl thrad caches to derive surrogate keys of dimensions which will be required duiring the insert of data into fact table.
			
				1)surrogate key ('sk' from now on) for every dimension used in feed will be exposed as  context attribute under name ${DIMENSION_NAME.sk}. it will have value of last line in the feed
				2)<exposeLastLineValueInContext> determines whether surrogate key caculated for the last line of every feed should be available in context as attribute named ${DIMENSION_NAME.sk}, default is false
				3)<cachePerThreadEnabled> caching per thread for dimension makes sense only if cardinality for that dimension is low. avoid caching for dimensions of high cardinalities (over million distinct values). or alternatively test both scenarios to derive performance statistiscs
				4) useInCombinedLookup should be used only for dimensions which do not have many values - at most few thousand. This helps engine to optimize access to those dimensions
				
				Following dimension types are supported INSERT_ONLY, T1, T2, CUSTOM.
				
				INSERT_ONLY-mode of operation assumes that all dimensional attributes are part of natural key. if exact match isn't found in cache inser statement will be issued and new generated surrogate key will be used. pelase see below
				
		-->
		<dimension name="dimension1" type="INSERT_ONLY" exposeLastLineValueInContext="false" cachePerThreadEnabled="true" useInCombinedLookup="true">
			<!-- define how many elements can be stored in local cache for this dimension (local etl thread cache (not global (Infinispan cache)).  if not set then default value will be used <dimension.local.cache.default.size>  -->	
			<localCacheMaxSize>10000</localCacheMaxSize>
			<mappedColumns>
				<!-- names of mappedColumns should match attributes in feed or attributes in context. Fastest is if they match attributes in feed -->
				<mappedColumn name="dataAttributeName_1" naturalKey="true" />
				<mappedColumn name="dataAttributeName_2" naturalKey="true" />
			</mappedColumns>
			<sqlStatements>
				<insertSingleRecord>
					insert into dimensionTableName1(dimensionSurrogateKey, tableAttribute1, tableAttribute2)
					values (dimensionSequenceName.nextval,'${dataAttributeName_1}','${dataAttributeName_2}')
				</insertSingleRecord>
				<!-- must return single value (surrogate key). Ideally WHERE clause should use all naturalKey columns from <mappedColumns> section -->
				<selectRecordIdentifier>select dimensionSurrogateKey from dimensionTableName1 where tableAttribute1='${dataAttributeName_1}' and tableAttribute2='${dataAttributeName_2}'</selectRecordIdentifier>
				<!--  order is important, first return surrogate key and then all natural keys, in order as defined by <mappedColumns> -->
				<!-- please make sure that cardinality of <preCacheRecords> resultset of sql query below is less than <localCacheMaxSize>. this is to avoid quick eviction of data from the cache. if not certan, please test both small and large <localCacheMaxSize> for dimensions with high cardinality (500K+)-->				
				<preCacheRecords>select dimensionSurrogateKey, tableAttribute1, tableAttribute2 from dimensionTableName1</preCacheRecords>
			</sqlStatements>
		</dimension>
		<!-- CUSTOM dimension type is backed fully by Java, for precaching and for lookup. There is still internal cache -->
		<dimension name="TEST_DIM" type="CUSTOM"> 
			<localCacheMaxSize>50000</localCacheMaxSize>
				<!-- full class name for data provider -->
				<dimensionDataProviderClassName>CustomDimensionDataProvider</dimensionDataProviderClassName>
				<!-- full class name for surrogate key provider -->
				<surrogateKeyProviderClassName>CustomSurrogateKeyProvider</surrogateKeyProviderClassName>
				<mappedColumns>
					<mappedColumn name="dataAttributeName_8" naturalKey="true" /> 
					<mappedColumn name="dataAttributeName_9" naturalKey="true" /> 
				</mappedColumns> 
		</dimension> 		
		<dimension name="dimension2" type="T2" exposeLastLineValueInContext="false" cachePerThreadEnabled="true">		
			<localCacheMaxSize>10000</localCacheMaxSize>
			<mappedColumns>
				<mappedColumn name="dataAttributeName_3" naturalKey="true" />
				<mappedColumn name="dataAttributeName_4" naturalKey="true" />
				<mappedColumn name="dataAttributeName_5" naturalKey="false" />
			</mappedColumns>
			<sqlStatements>
				<insertSingleRecord>
					insert into dimensionTableName2(dimensionSurrogateKey, tableAttribute1, tableAttribute2, dataAttributeName_41, valid)
					values (dimensionSequenceName.nextval,'${dataAttributeName_1}','${dataAttributeName_2}', 'dataAttributeName_41', 'Y')
				</insertSingleRecord>
				<selectRecordIdentifier>select dimensionSurrogateKey from dimensionTableName2 where tableAttribute1='${dataAttributeName_1}' and tableAttribute2='${dataAttributeName_2}' and valid='Y'</selectRecordIdentifier>
				<preCacheRecords>select dimensionSurrogateKey, tableAttribute1, tableAttribute2 from dimensionTableName2 where valid='Y'</preCacheRecords>
				<retireSingleRecord>update dimensionTableName2 set valid='N' where dataAttributeName_3='${dataAttributeName_3}' and dataAttributeName_4='${dataAttributeName_4}' and valid='Y'</retireSingleRecord>
			</sqlStatements>
		</dimension>		
		<dimension name="dimension3" type="T1" exposeLastLineValueInContext="false" cachePerThreadEnabled="true">
			<!-- class implementing com.threeglav.sh.bauk.dimension.DimensionDataProvider interface -->
			<!-- returns additional mappings from natural to surrogate key for this dimension -->
			<dimensionDataProviderClassName>CustomDimensionDataProvider</dimensionDataProviderClassName>
			<localCacheMaxSize>10000</localCacheMaxSize>
			<mappedColumns>
				<mappedColumn name="dataAttributeName_5" naturalKey="true" />
				<mappedColumn name="dataAttributeName_6" naturalKey="true" />
				<mappedColumn name="dataAttributeName_7" naturalKey="false" />
				<mappedColumn name="dataAttributeName_8" naturalKey="false" />
			</mappedColumns>
			<sqlStatements>
				<insertSingleRecord>
					insert into dimensionTableName3(dimensionSurrogateKey, tableAttribute1, tableAttribute2)
					values (dimensionSequenceName.nextval,'${dataAttributeName_1}','${dataAttributeName_2}')
				</insertSingleRecord>
				<updateSingleRecord>update dimensionTableName3 set dataAttributeName_7='${dataAttributeName_7}', dataAttributeName_8='${dataAttributeName_8}' where dataAttributeName_5='${dataAttributeName_5}' and dataAttributeName_6='${dataAttributeName_6}'</updateSingleRecord>
				<selectRecordIdentifier>select dimensionSurrogateKey from dimensionTableName3 where tableAttribute1='${dataAttributeName_1}' and tableAttribute2='${dataAttributeName_2}'</selectRecordIdentifier>
				<preCacheRecords>select dimensionSurrogateKey, tableAttribute1, tableAttribute2 from dimensionTableName3</preCacheRecords>
			</sqlStatements>
		</dimension>
	</dimensions>
	<properties>
		<!-- should processed feed files be remembered or not (true/false). this setting essentially disables system to process input feed file with the same name twice if set to true -->
		<property name="idempotent.feed.processing">false</property>
		<!-- what is used for caching, choices are Hazelcast or Infinispan, values of parameters should be hazelcast or ispn. if global <cache.provider> doesn't contain lookup value database will be queried (insert will be tried, if fails, adequate key will be read) -->
		<property name="cache.provider">ispn</property>
		<!-- default local cache size for dimensions. Can be overridden for every dimension (see below dimension configuration section). Local cache is size of local collection each etl thread has. etl thread has one cache for every single dimension defined. if cache has no value local etl thread cache will query global <cache.provider> (Infinispan or Hazelcast) -->
		<property name="dimension.local.cache.default.size">5000</property>
		<!-- read buffer size to disk (megabytes) - accepts decimal numbers. adjust this value to be just over the size of your biggest source feed file. if you experience IO bottleneck adjusting to size of 10% up to 100% of source feed file could bring performance benefits -->
		<property name="read.buffer.size.mb">10.0</property>
		<!-- write buffer size to disk (megabytes) - accepts decimal numbers. adjust this value to be just over the size of your biggest source feed file. if you experience IO bottleneck adjusting to size of 10% up to 100% of source feed file could bring performance benefits -->
		<property name="write.buffer.size.mb">10.0</property>
		<!-- whether local cache for dimensions (local cache of etl threads) should be disabled (true/false). -->
		<property name="dimension.local.cache.disable">false</property>
		<!-- is exposing statistics feature off or on (true/false). No need to disable except in case when box is CPU bound -->
		<property name="metrics.off">false</property>
		<!-- jdbc client info - program name, set to empty not to use this feature. Setting embeds client information to simplify database connection audit and tuning -->
		<property name="jdbc.client.info.program"></property>
		<!-- indicates whether files moved from source to archive folder should be renamed (renaming will result in appending date and time to the file name) -->
		<property name="rename.archived.files">false</property>
		<!-- should we detect other running engine instances and print count at startup -->
		<property name="detect.running.engine.instances">false</property>
		<!-- JDBC fetch size used when pre-caching dimension keys. Must be positive integer value. Dimension pre-caching is used by global cache.provider at startup in order to reduce impact on database at runtime and avoid performance degradation. pre-caching populates Infinispan or Hazelcast global cache with dimensional keys & attributes.-->
		<property name="dimension.precache.jdbc.fetch.size">10000</property>
		<!-- whether pre-caching for all dimensions is disabled for both global <cache.provider> (Infinispan) and etl threads <feedProcessingThreads> -->
		<property name="dimension.precaching.disabled">false</property>
		<!-- should combined lookup feature for low cardinality dimensions be disabled -->
		<property name="disable.dimension.combined.lookup">true</property>
		<!-- number of locks used when updating slowly-changing dimensions. Must be positive integer -->
		<property name="scd.lock.striping.lock.count">50</property>
		<!-- should processing statistics be written to log file and console as files are processed -->
		<property name="output.processing.statistics">false</property>
		<!-- should average statistics be written to log file and console when engine is shut down -->
		<property name="output.final.average.statistics">false</property>
		<!-- when persisting NULL value what should actually be written to output (bulk) file, by default this is empty string (no character will be written) -->
		<property name="bulk.output.file.null.value.string"></property>
		<!-- should bulk files be deleted after they are loaded. if set to true, bulk files will be deleted from bulkOutputDirectory -->
		<property name="bulk.delete.files.after.load">true</property>
		<!-- should engine record (internally) submission of every file it tries to bulk load? this setting is used for recovery, in case when engine dies halfway through bulk-loading file it will know that file was submitted for loading beforehand (this info is persisted and it survives engine crash and/or restart) -->
		<property name="bulk.file.record.submission.attempts">false</property>
		<!-- how often (in milliseconds) to poll for input files (feed, bulk). -->
		<property name="file.polling.delay.millis">1000</property>
		<!-- following two properties can be 0 if file system allows atomic renames/moves of files. Data corruption can occur in case someone is writing to files while engine is consuming them. this can be prevented by user (files can be renamed by file writer after writing is done) -->
		<!-- how old must be feed file before it can be accepted for processing. set to 0 to immediately process files -->
		<property name="feed.file.acceptance.timeout.millis">2000</property>
		<!-- how old must be bulk file before it can be accepted for processing. set to 0 to immediately process files -->
		<property name="bulk.file.acceptance.timeout.millis">2000</property>
		<!-- batch size to be used when bulk loading using jdbc. Whenever number of batched values reaches this number it will be executed, committed and batching will start again -->
		<property name="jdbc.bulk.loading.batch.size">10000</property>
		<!-- applicable only when using JDBC output type. Ensures that identifiers assigned to ETL threads are only in range [0 - (partition.count-1)] regardless how many ETL threads we have. If set to <=0 partitions will not be used -->
		<property name="jdbc.threads.partition.count">-1</property>
		<!-- applicable to DB threads when performing bulk loading (not when ETL threads are doing inserts like: pipes and JDBC). Ensures that identifiers bulkProcessingPartitionedThreadID assigned to DB threads are only in range [0 - (partition.count-1)] regardless how many DB threads we have -->
		<!-- if set to <=0 partitions will not be used -->
		<property name="bulk.load.threads.partition.count">-1</property>
		<!-- throughput.testing.mode eliminates read I/O bottleneck while testing. when set to true, etl threads <feedProcessingThreads> will each cache one random file and process it not by reading it from disk but by reading it from repeatedly (while test lasts) from internal buffer. this will give idea of throughput without read I/O bottleneck. it is advisable that source directory contains more files than number of etl threads <feedProcessingThreads> -->
		<property name="throughput.testing.mode">false</property>
		<!-- port to be used by http server for remote commands. If set to 0 then http server will not be started.
			if something else than engine engine has updated database dimensional values that implies that global cache (Infinispan) has incorrect values. 
			you could make engine flush all dimensional caches if you execute http://localhost:21000/flushDimensionCache/?dimension=dimension1
		-->
		<property name="remoting.server.port">21000</property>
		<!-- used for email feature -->
		<property name="email.server.host">emailServerHostName</property>
		<property name="email.server.username"></property>
		<property name="email.server.password"></property>
		<property name="email.server.port">25</property>
		<!-- addressses to which error emails will be sent (coma delimited list of email addresses)-->
		<property name="processing.error.email.recipients">a@b.com,c@d.com</property>
	</properties>
</config>